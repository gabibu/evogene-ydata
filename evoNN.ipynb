{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gabiburabia24/.local/lib/python3.5/site-packages/sklearn/externals/six.py:31: DeprecationWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n",
      "  \"(https://pypi.org/project/six/).\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "from skmultilearn.problem_transform import LabelPowerset\n",
    "from collections import Counter\n",
    "from sklearn.datasets import make_classification\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.HDFStore('Y1.h5',  mode='r') as store:\n",
    "    Y = store.select('Y')\n",
    "\n",
    "with pd.HDFStore('X1.h5',  mode='r') as store:\n",
    "    X = store.select('X')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y.sort_values('cluster', inplace=True)\n",
    "X.sort_values('cluster_is', inplace=True)\n",
    "\n",
    "Y.drop('cluster', inplace=True,axis=1)\n",
    "X.drop(['cluster_is'], inplace=True,axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['A', 'Acidic', 'Aliphatic', 'Aromatic', 'Basic', 'C', 'Charged', 'D',\n",
       "       'E', 'F', 'G', 'H', 'I', 'InclusionBodiesProbability', 'K', 'L', 'M',\n",
       "       'N', 'Non-polar', 'P', 'Polar', 'Q', 'R', 'S', 'Small', 'T', 'Tiny',\n",
       "       'V', 'W', 'Y', 'codone_usage', 'gc_std', 'kaks', 'length', 'loc_type',\n",
       "       'localization', 'mol_w', 'pi', 'pseb_H1_l_1', 'pseb_H1_l_10',\n",
       "       'pseb_H1_l_2', 'pseb_H1_l_3', 'pseb_H1_l_4', 'pseb_H1_l_5',\n",
       "       'pseb_H1_l_6', 'pseb_H1_l_7', 'pseb_H1_l_8', 'pseb_H1_l_9',\n",
       "       'pseb_H2_l_11', 'pseb_H2_l_12', 'pseb_H2_l_13', 'pseb_H2_l_14',\n",
       "       'pseb_H2_l_15', 'pseb_H2_l_16', 'pseb_H2_l_17', 'pseb_H2_l_18',\n",
       "       'pseb_H2_l_19', 'pseb_H2_l_20', 'relative_codone_usage', 'repeated',\n",
       "       'tmlen'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "61"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2528843, 61)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for index, data in X.iterrows():\n",
    "#     pass\n",
    "\n",
    "# print(index)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2528843, (2528843, 61))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# len(X), X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = np.empty(shape=(X.shape[0], 69))\n",
    "\n",
    "for index, (_, data) in enumerate(X.iterrows()):\n",
    "    features[index][0] = data['A']\n",
    "    features[index][1] = data['Acidic']\n",
    "    features[index][2] = data['Aliphatic']\n",
    "    features[index][3] = data['Aromatic']\n",
    "    features[index][4] = data['Basic']\n",
    "    features[index][5] = data['C']\n",
    "    features[index][6] = data['K']\n",
    "    \n",
    "    \n",
    "    \n",
    "    features[index][7] = data['Charged']\n",
    "    features[index][8] = data['D']\n",
    "    features[index][9] = data['E']\n",
    "    features[index][10] = data['F']\n",
    "    features[index][11] = data['G']\n",
    "    features[index][12] = data['H']\n",
    "    features[index][13] = data['I']\n",
    "    features[index][14] = data['InclusionBodiesProbability']\n",
    "    features[index][15] = data['L']\n",
    "    features[index][16] = data['M']\n",
    "    features[index][17] = data['N']\n",
    "    \n",
    "    features[index][18] = data['Non-polar']\n",
    "    features[index][19] = data['P']\n",
    "    features[index][20] = data['Polar']\n",
    "    features[index][21] = data['Q']\n",
    "    features[index][22] = data['R']\n",
    "    \n",
    "    features[index][23] = data['S']\n",
    "    features[index][24] = data['Small']\n",
    "    features[index][25] = data['T']\n",
    "    \n",
    "    features[index][26] = data['Tiny']\n",
    "    features[index][27] = data['V']\n",
    "    features[index][28] = data['W']\n",
    "    features[index][29] = data['Y']\n",
    "    features[index][30] = data['codone_usage']\n",
    "    features[index][31] = data['gc_std']\n",
    "    features[index][32] = data['kaks']\n",
    "    features[index][33] = data['length']\n",
    "    \n",
    "    location_type = data['loc_type']\n",
    "    \n",
    "    features[index][34] = 0\n",
    "    features[index][35] = 0\n",
    "    features[index][36] = 0\n",
    "    features[index][37] = 0\n",
    "    features[index][38] = 0\n",
    "    features[index][39] = 0\n",
    "    features[index][40] = 0\n",
    "    features[index][41] = 0\n",
    "    features[index][42] = 0\n",
    "    \n",
    "    if location_type == '0':\n",
    "        features[index][34] = 1 \n",
    "    elif location_type == 'SignalP_gram_plus_S':\n",
    "        features[index][35] = 1 \n",
    "    elif location_type == 'SignalP_gram_minus_S':\n",
    "        features[index][36] = 1 \n",
    "    elif location_type == 'T4SEpre_bpbAac_T4SE':\n",
    "        features[index][37] = 1 \n",
    "    elif location_type == 'hmmsearch_Tat':\n",
    "        features[index][38] = 1 \n",
    "    elif location_type == 'T3_MM_T3SS':\n",
    "        features[index][39] = 1 \n",
    "    elif location_type == 'SignalP_gram_plus_S':\n",
    "        features[index][40] = 1 \n",
    "    elif location_type == 'hmmsearch_T6S':\n",
    "        features[index][41] = 1 \n",
    "    elif location_type == 'hmmsearch_T1S':\n",
    "        features[index][42] = 1 \n",
    "    else:\n",
    "        raise Exception(location_type)\n",
    "    \n",
    "    \n",
    "    features[index][43] = data['localization']\n",
    "    features[index][44] = data['mol_w']\n",
    "    features[index][45] = data['pi']\n",
    "    features[index][46] = data['pseb_H1_l_1']\n",
    "    features[index][47] = data['pseb_H1_l_10']\n",
    "    features[index][48] = data['pseb_H1_l_2']\n",
    "    features[index][49] = data['pseb_H1_l_3']\n",
    "    \n",
    "    features[index][50] = data['pseb_H1_l_4']\n",
    "    features[index][51] = data['pseb_H1_l_5']\n",
    "    features[index][52] = data['pseb_H1_l_6']\n",
    "    features[index][53] = data['pseb_H1_l_7']\n",
    "    features[index][54] = data['pseb_H1_l_8']\n",
    "    features[index][55] = data['pseb_H1_l_9']\n",
    "    features[index][56] = data['pseb_H2_l_11']\n",
    "    features[index][57] = data['pseb_H2_l_12']\n",
    "    features[index][58] = data['pseb_H2_l_13']\n",
    "    features[index][59] = data['pseb_H2_l_14']\n",
    "    \n",
    "    features[index][60] = data['pseb_H2_l_15']\n",
    "    features[index][61] = data['pseb_H2_l_16']\n",
    "    features[index][62] = data['pseb_H2_l_17']\n",
    "    features[index][63] = data['pseb_H2_l_18']\n",
    "    features[index][64] = data['pseb_H2_l_19']\n",
    "    features[index][65] = data['pseb_H2_l_20']\n",
    "    features[index][66] = data['relative_codone_usage']\n",
    "    features[index][67] = data['repeated']\n",
    "    features[index][68] = data['tmlen']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"features.txt\", features, fmt=\"%s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = np.loadtxt(\"features.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "scaler = preprocessing.StandardScaler()\n",
    "scalled_features = scaler.fit_transform(features)\n",
    "# scalled_features = features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X['InclusionBodiesProbPos'] = (X['InclusionBodiesProbability']>=0)*X['InclusionBodiesProbability']\n",
    "# X['InclusionBodiesProbNeg'] = (X['InclusionBodiesProbability']<0)*X['InclusionBodiesProbability']\n",
    "\n",
    "\n",
    "# def location_type_numeric(x1):\n",
    "#     if x1 == '0':\n",
    "#         return 1\n",
    "#     elif x1 == 'SignalP_gram_plus_S':\n",
    "#         return 2\n",
    "#     elif x1 == 'SignalP_gram_minus_S':\n",
    "#         return 3\n",
    "#     elif x1 == 'T4SEpre_bpbAac_T4SE':\n",
    "#         return 4\n",
    "#     elif x1 == 'hmmsearch_Tat':\n",
    "#         return 5\n",
    "#     elif x1 == 'T3_MM_T3SS':\n",
    "#         return 6\n",
    "#     elif x1 == 'SignalP_gram_plus_S':\n",
    "#         return 7\n",
    "#     elif x1 == 'hmmsearch_T6S':\n",
    "#         return 8\n",
    "#     elif x1 == 'hmmsearch_T1S':\n",
    "#         return 9\n",
    "#     else:\n",
    "#         raise Exception(x1)\n",
    "\n",
    "# X['loc_type_numeric'] = X['loc_type'].apply(location_type_numeric)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Y.sort_values('cluster', inplace=True)\n",
    "# X.sort_values('cluster_is', inplace=True)\n",
    "\n",
    "# Y.drop('cluster', inplace=True,axis=1)\n",
    "# X.drop(['cluster_is', 'InclusionBodiesProbability', 'loc_type' ], inplace=True,axis=1)\n",
    "\n",
    "\n",
    "\n",
    "# number_neg_samples / number_pos_samples "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_on_sums = Y.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6688"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# labels_on_sums.sort_values(ascending=False)["
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2528843, 425)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_on_sums = labels_on_sums[labels_on_sums >= 1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_columns = [col for col in Y.columns if col not in labels_on_sums.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['go_9773', 'go_42256', 'go_6003', 'go_19877']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_columns[0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "245"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.shape[1] - len(remove_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y.drop(remove_columns, inplace=True,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "used_labels_on_sums = Y.sum(axis=0)\n",
    "weights = ((Y.shape[0] - used_labels_on_sums) /  used_labels_on_sums).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 29.34660155,  17.86351634,  13.97792559, 343.38826093]),\n",
       " array([1458.22850548, 1128.45198749, 1134.02827648]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights[0:4], weights[-4:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "245"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2528843, 245), (2528843, 69))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.shape, scalled_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(scalled_features, Y.values, test_size=0.3)\n",
    "# X_test, X_validation, y_test, y_validation = train_test_split(X_test, y_test, test_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(758653, 27)"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lp = LabelPowerset()\n",
    "# yt = lp.transform(y_train)\n",
    "# rus = RandomUnderSampler(random_state=42)\n",
    "# X_res, y_res = rus.fit_resample(X_train, yt)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_res = lp.inverse_transform(y_res)\n",
    "# y_res = y_res.todense()\n",
    "# y_res =np.array(y_res)\n",
    "\n",
    "# X_res = X_train.values\n",
    "# y_res = y_train.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DatasetLoader(Dataset):\n",
    "\n",
    "    def __init__(self, x_df, y_df):\n",
    "        self.x_df = x_df\n",
    "        self.y_df = y_df\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x_df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.x_df[idx]\n",
    "        y = self.y_df[idx]\n",
    "\n",
    "        return x.reshape(-1), y.reshape(-1)\n",
    "\n",
    "\n",
    "\n",
    "train_dataset_loader = DatasetLoader(X_train, y_train)\n",
    "train_loader = DataLoader(train_dataset_loader, batch_size=100, shuffle=True, num_workers=0)\n",
    "\n",
    "\n",
    "test_dataset_loader = DatasetLoader(X_test, y_test)\n",
    "test_loader = DataLoader(test_dataset_loader, batch_size=100, shuffle=True, num_workers=0)\n",
    "\n",
    "class GoTermNN(nn.Module):\n",
    "    def __init__(self, num_classes, num_inputs=69,  dropout_prob=0.3):\n",
    "        super(GoTermNN, self).__init__()\n",
    "        self.pipe = nn.Sequential(\n",
    "            nn.Linear(num_inputs, 200),\n",
    "            nn.Sigmoid(),\n",
    "            \n",
    "            nn.Linear(200, 100),\n",
    "            nn.Sigmoid(),\n",
    "            \n",
    "            nn.Linear(100, num_classes)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.pipe(x)\n",
    "    \n",
    "class GoTermNN2(nn.Module):\n",
    "    def __init__(self, num_classes, num_inputs=69,  dropout_prob=0.3):\n",
    "        super(GoTermNN2, self).__init__()\n",
    "        self.pipe = nn.Sequential(\n",
    "            nn.Linear(num_inputs, 500),\n",
    "            nn.Sigmoid(),\n",
    "            \n",
    "            nn.Linear(500, 400),\n",
    "            nn.Sigmoid(),\n",
    "            \n",
    "            nn.Linear(400, 300),\n",
    "            nn.Sigmoid(),\n",
    "            \n",
    "            nn.Linear(300, num_classes)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.pipe(x)\n",
    "\n",
    "# class GoTermNN(nn.Module):\n",
    "#     def __init__(self, num_classes, num_inputs=62,  dropout_prob=0.3):\n",
    "#         super(GoTermNN, self).__init__()\n",
    "#         self.pipe = nn.Sequential(\n",
    "#             nn.Linear(num_inputs, 40),\n",
    "#             nn.Tanh(),\n",
    "            \n",
    "#             nn.Linear(40, 30),\n",
    "#             nn.Tanh(),\n",
    "            \n",
    "#             nn.Linear(30, 20),\n",
    "            \n",
    "#             nn.Tanh(),\n",
    "\n",
    "#             nn.Linear(20, num_classes)\n",
    "#         )\n",
    "\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return self.pipe(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, epoch, writer, criterion):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_correct = 0.0\n",
    "    calc_count = 0.0\n",
    "    loss1 = 0.1\n",
    "\n",
    "    per_label_expected = None\n",
    "    per_label_predicted = None\n",
    "\n",
    "    for x, (images, labels) in enumerate(train_loader):\n",
    "\n",
    "        if per_label_expected is None:\n",
    "            per_label_expected=[]\n",
    "            per_label_predicted = []\n",
    "\n",
    "            for i in range(labels.shape[0]):\n",
    "                per_label_expected.append([])\n",
    "                per_label_predicted.append([])\n",
    "\n",
    "\n",
    "        images = images.float().to(device)\n",
    "        labels = labels.float().to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        images = Variable(images)\n",
    "        labels = Variable(labels)\n",
    "\n",
    "        output = model(images)\n",
    "        calc_count += output.data.shape[0]\n",
    "        _, pred = torch.max(output.data, 1)\n",
    "        loss = criterion(output, labels)\n",
    "\n",
    "        current_loss = loss.item()\n",
    "        running_loss += current_loss\n",
    "\n",
    "        loss1 += current_loss\n",
    "\n",
    "        if x > 0 and  x %10000 == 0:\n",
    "            print('train loss {0}'.format(current_loss))\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    return round(running_loss / calc_count, 4)\n",
    "\n",
    "\n",
    "def test(model, device, test_loader, epoch, writer, criterion):\n",
    "    model.eval()\n",
    "    running_correct = 0.0\n",
    "    running_loss = 0.0\n",
    "    calc_count = 0.0\n",
    "\n",
    "    per_label_expected = None\n",
    "    per_label_predicted = None\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "\n",
    "            if per_label_expected is None:\n",
    "                per_label_expected = []\n",
    "                per_label_predicted = []\n",
    "\n",
    "                for i in range(labels.shape[0]):\n",
    "                    per_label_expected.append([])\n",
    "                    per_label_predicted.append([])\n",
    "\n",
    "            images = images.float().to(device)\n",
    "            labels = labels.float().to(device)\n",
    "\n",
    "            images = Variable(images)\n",
    "            labels = Variable(labels)\n",
    "            outputs = model(images)\n",
    "\n",
    "\n",
    "            #_, pred = torch.max(outputs.data, 1)\n",
    "            loss = criterion(outputs, labels)\n",
    "            calc_count += outputs.data.shape[0]\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "    print('validation loss {0}'.format(running_loss/calc_count))\n",
    "    \n",
    "    return running_loss/calc_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "use_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use_cuda = True\n",
      "epoch=0\n",
      "train loss 0.620721161365509\n",
      "epoch=1\n",
      "train loss 0.7164129614830017\n",
      "epoch=2\n",
      "train loss 0.5140013098716736\n",
      "epoch=3\n",
      "train loss 0.6842884421348572\n",
      "epoch=4\n",
      "train loss 0.35757437348365784\n",
      "epoch=5\n",
      "train loss 0.590869665145874\n",
      "epoch=6\n",
      "train loss 0.48750895261764526\n",
      "epoch=7\n",
      "train loss 0.5573505163192749\n",
      "epoch=8\n",
      "train loss 0.7100915908813477\n",
      "epoch=9\n",
      "train loss 0.7810368537902832\n",
      "epoch=10\n",
      "train loss 0.4189562201499939\n",
      "epoch=11\n",
      "train loss 0.5703253149986267\n",
      "epoch=12\n",
      "train loss 0.6032425761222839\n",
      "epoch=13\n",
      "train loss 0.3989981710910797\n",
      "epoch=14\n",
      "train loss 0.4227091073989868\n",
      "epoch=15\n",
      "train loss 0.4263768494129181\n",
      "epoch=16\n",
      "train loss 1.0190343856811523\n",
      "epoch=17\n",
      "train loss 0.55959552526474\n",
      "epoch=18\n",
      "train loss 0.4517976939678192\n",
      "epoch=19\n",
      "train loss 0.7039374113082886\n",
      "epoch=20\n",
      "train loss 0.7372170090675354\n",
      "epoch=21\n",
      "train loss 0.5937597155570984\n",
      "epoch=22\n",
      "train loss 0.5225681066513062\n",
      "epoch=23\n",
      "train loss 0.6085909605026245\n",
      "epoch=24\n",
      "train loss 0.32976171374320984\n",
      "epoch=25\n",
      "train loss 1.011596918106079\n",
      "epoch=26\n",
      "train loss 0.8794891238212585\n",
      "epoch=27\n",
      "train loss 0.5379120111465454\n",
      "epoch=28\n",
      "train loss 0.4587683081626892\n",
      "epoch=29\n",
      "train loss 0.7421591877937317\n",
      "epoch=30\n",
      "train loss 0.690322995185852\n",
      "epoch=31\n",
      "train loss 0.43621116876602173\n",
      "epoch=32\n",
      "train loss 0.855965256690979\n",
      "epoch=33\n",
      "train loss 0.33203214406967163\n",
      "epoch=34\n",
      "train loss 0.6912947297096252\n",
      "epoch=35\n",
      "train loss 0.6760537624359131\n",
      "epoch=36\n",
      "train loss 0.5016571879386902\n",
      "epoch=37\n",
      "train loss 0.7762264013290405\n",
      "epoch=38\n",
      "train loss 0.6314640641212463\n",
      "epoch=39\n",
      "train loss 0.422835111618042\n",
      "epoch=40\n",
      "train loss 0.7141067385673523\n",
      "epoch=41\n",
      "train loss 0.6116960644721985\n",
      "epoch=42\n",
      "train loss 0.5973933935165405\n",
      "epoch=43\n",
      "train loss 0.5474100112915039\n",
      "epoch=44\n",
      "train loss 0.363648921251297\n",
      "epoch=45\n",
      "train loss 0.3980298638343811\n",
      "epoch=46\n",
      "train loss 0.5556821227073669\n",
      "epoch=47\n",
      "train loss 0.6875832080841064\n",
      "epoch=48\n",
      "train loss 0.518917441368103\n",
      "epoch=49\n",
      "train loss 0.5206625461578369\n",
      "epoch=50\n",
      "train loss 0.5796566605567932\n",
      "epoch=51\n",
      "train loss 0.6590479612350464\n",
      "epoch=52\n",
      "train loss 0.5859077572822571\n",
      "epoch=53\n",
      "train loss 0.517162561416626\n",
      "epoch=54\n",
      "train loss 0.5967748761177063\n",
      "epoch=55\n",
      "train loss 0.7451273798942566\n",
      "epoch=56\n",
      "train loss 0.6366106867790222\n",
      "epoch=57\n",
      "train loss 0.43789952993392944\n",
      "epoch=58\n",
      "train loss 0.5573944449424744\n",
      "epoch=59\n",
      "train loss 0.6125320792198181\n",
      "epoch=60\n",
      "train loss 0.5307100415229797\n",
      "epoch=61\n",
      "train loss 0.7966817021369934\n",
      "epoch=62\n",
      "train loss 0.5249983072280884\n",
      "epoch=63\n",
      "train loss 0.6180797815322876\n",
      "epoch=64\n",
      "train loss 0.6153069734573364\n",
      "epoch=65\n",
      "train loss 0.6197497844696045\n",
      "epoch=66\n",
      "train loss 0.5455881953239441\n",
      "epoch=67\n",
      "train loss 0.7653826475143433\n",
      "epoch=68\n",
      "train loss 0.47392532229423523\n",
      "epoch=69\n",
      "train loss 0.9391784071922302\n",
      "epoch=70\n",
      "train loss 0.6902362108230591\n",
      "epoch=71\n",
      "train loss 0.46940991282463074\n",
      "epoch=72\n",
      "train loss 0.6144420504570007\n",
      "epoch=73\n",
      "train loss 0.677838146686554\n",
      "epoch=74\n",
      "train loss 0.5420059561729431\n",
      "epoch=75\n",
      "train loss 0.4067094027996063\n",
      "epoch=76\n",
      "train loss 0.7471823692321777\n",
      "epoch=77\n",
      "train loss 0.6829280257225037\n",
      "epoch=78\n",
      "train loss 0.8522739410400391\n",
      "epoch=79\n",
      "train loss 0.6751253008842468\n",
      "epoch=80\n",
      "train loss 0.7623563408851624\n",
      "epoch=81\n",
      "train loss 0.5682384967803955\n",
      "epoch=82\n",
      "train loss 0.6694034934043884\n",
      "epoch=83\n",
      "train loss 0.485909640789032\n",
      "epoch=84\n",
      "train loss 0.5907861590385437\n",
      "epoch=85\n",
      "train loss 0.7165027260780334\n",
      "epoch=86\n",
      "train loss 0.529809832572937\n",
      "epoch=87\n",
      "train loss 0.7274698615074158\n",
      "epoch=88\n",
      "train loss 0.6438134908676147\n",
      "epoch=89\n",
      "train loss 0.6387820243835449\n",
      "epoch=90\n",
      "train loss 0.4751676023006439\n",
      "epoch=91\n",
      "train loss 0.47862815856933594\n",
      "epoch=92\n",
      "train loss 0.5875653624534607\n",
      "epoch=93\n",
      "train loss 0.576184868812561\n",
      "epoch=94\n",
      "train loss 0.8560787439346313\n",
      "epoch=95\n",
      "train loss 0.837543249130249\n",
      "epoch=96\n",
      "train loss 0.6460273861885071\n",
      "epoch=97\n",
      "train loss 0.5938639640808105\n",
      "epoch=98\n",
      "train loss 0.5214372277259827\n",
      "epoch=99\n",
      "train loss 1.1653892993927002\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "torch.manual_seed(999)\n",
    "\n",
    "if use_cuda:\n",
    "    torch.cuda.manual_seed(999)\n",
    "    torch.cuda.manual_seed_all(999)\n",
    "\n",
    "print('use_cuda = {}'.format(use_cuda))\n",
    "\n",
    "\n",
    "weights_trensor = torch.tensor(weights).float() #.to(device)\n",
    "criterion = torch.nn.BCEWithLogitsLoss(pos_weight = weights_trensor).to(device)\n",
    "\n",
    "torch.manual_seed(999)\n",
    "now = datetime.now()\n",
    "\n",
    "#model = GoTermNN(y_train.shape[1]).to(device)\n",
    "model = GoTermNN2(y_train.shape[1]).to(device)\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "#optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "\n",
    "for epoch in range(100):\n",
    "    print('epoch={}'.format(epoch))\n",
    "    train_loss = train(model, device, train_loader, optimizer, epoch, None, criterion)\n",
    "    #test_loss = test(model, device, test_loader, epoch, None, criterion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def trainCycle(model, optimizer, num_of_epochs):\n",
    "    \n",
    "    torch.manual_seed(999)\n",
    "\n",
    "    if use_cuda:\n",
    "        torch.cuda.manual_seed(999)\n",
    "        torch.cuda.manual_seed_all(999)\n",
    "\n",
    "    print('use_cuda = {}'.format(use_cuda))\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "    weights_trensor = torch.tensor(weights).float() #.to(device)\n",
    "    criterion = torch.nn.BCEWithLogitsLoss(pos_weight = weights_trensor).to(device)\n",
    "\n",
    "    torch.manual_seed(999)\n",
    "    now = datetime.now()\n",
    "\n",
    "    #model = GoTermNN(y_train.shape[1]).to(device)\n",
    "    #model = GoTermNN2(y_train.shape[1]).to(device)\n",
    "\n",
    "\n",
    "    #optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    #optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "\n",
    "    for epoch in range(num_of_epochs):\n",
    "        print('epoch={}'.format(epoch))\n",
    "        train_loss = train(model, device, train_loader, optimizer, epoch, None, criterion)\n",
    "        #test_loss = test(model, device, test_loader, epoch, None, criterion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use_cuda = True\n",
      "epoch=0\n",
      "train loss 0.2455453872680664\n",
      "epoch=1\n",
      "train loss 0.1687076985836029\n",
      "epoch=2\n",
      "train loss 0.12218335270881653\n",
      "epoch=3\n",
      "train loss 0.0780547559261322\n",
      "epoch=4\n",
      "train loss 0.08905154466629028\n",
      "epoch=5\n",
      "train loss 0.06795430928468704\n",
      "epoch=6\n",
      "train loss 0.06929603219032288\n",
      "epoch=7\n",
      "train loss 0.05796745792031288\n",
      "epoch=8\n",
      "train loss 0.07774253189563751\n",
      "epoch=9\n",
      "train loss 0.045303355902433395\n",
      "epoch=10\n",
      "train loss 0.058212827891111374\n",
      "epoch=11\n",
      "train loss 0.06949786841869354\n",
      "epoch=12\n",
      "train loss 0.06604993343353271\n",
      "epoch=13\n",
      "train loss 0.12763074040412903\n",
      "epoch=14\n",
      "train loss 0.09772027283906937\n",
      "epoch=15\n",
      "train loss 0.11091586947441101\n",
      "epoch=16\n",
      "train loss 0.1741185039281845\n",
      "epoch=17\n",
      "train loss 0.07061723619699478\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-41da3ce6c534>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGoTermNN2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrainCycle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimizer\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-22-0e8ef87b9641>\u001b[0m in \u001b[0;36mtrainCycle\u001b[0;34m(model, optimizer, num_of_epochs)\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_of_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'epoch={}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0;31m#test_loss = test(model, device, test_loader, epoch, None, criterion)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-89be21bccadb>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, device, train_loader, optimizer, epoch, writer, criterion)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrunning_loss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mcalc_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m                 \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m                 \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m                 \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mamsgrad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = GoTermNN2(y_train.shape[1]).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "trainCycle(model,optimizer , 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'GoTermNN21000_all_labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GoTermNN2(\n",
       "  (pipe): Sequential(\n",
       "    (0): Linear(in_features=69, out_features=500, bias=True)\n",
       "    (1): Sigmoid()\n",
       "    (2): Linear(in_features=500, out_features=400, bias=True)\n",
       "    (3): Sigmoid()\n",
       "    (4): Linear(in_features=400, out_features=300, bias=True)\n",
       "    (5): Sigmoid()\n",
       "    (6): Linear(in_features=300, out_features=245, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GoTermNN2(y_train.shape[1]).to(device)\n",
    "model.load_state_dict(torch.load('GoTermNN21000_1'))\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chaiclassifier.ipynb\t\t\t\t    GoTermNN21000\r\n",
      "cuda-repo-ubuntu1604-9-0-local_9.0.176-1_amd64-deb  multilabelanalysis.ipynb\r\n",
      "evo.zip\t\t\t\t\t\t    NN.ipynb\r\n",
      "features.txt\t\t\t\t\t    X1.h5\r\n",
      "GoTermNN2\t\t\t\t\t    Y1.h5\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for  index, (images, labels) in enumerate(test_loader):\n",
    "        images = images.float().to(device)\n",
    "        labels = labels.float().to(device)\n",
    "        images = Variable(images)\n",
    "        labels = Variable(labels)\n",
    "        outputs = model(images)\n",
    "        probs = torch.sigmoid(outputs)\n",
    "        #print(probs.shape)\n",
    "        \n",
    "        #for feature in probs.shape[1]:\n",
    "        \n",
    "        if index > 5:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcStat(model, threshold, label_index, loader):\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "    all_scores = []\n",
    "    for  index, (images, labels) in enumerate(loader):\n",
    "        images = images.float().to(device)\n",
    "        labels = labels.float().to(device)\n",
    "        images = Variable(images)\n",
    "        labels = Variable(labels)\n",
    "        outputs = model(images)\n",
    "        probs = torch.sigmoid(outputs)\n",
    "        my_labels = labels[:, label_index]\n",
    "        my_probs = probs[:, label_index]\n",
    "        \n",
    "        probs = probs.cpu().numpy()\n",
    "        \n",
    "        \n",
    "        all_scores.extend(probs[:, label_index].tolist())\n",
    "        results = np.zeros(shape=probs[:, label_index].shape)\n",
    "        results[np.where(probs[:, label_index] >= threshold)] = 1\n",
    "        all_predictions.extend(results.tolist())\n",
    "        \n",
    "        all_labels.extend(labels[:,label_index].tolist())\n",
    "        \n",
    "    return all_labels, all_predictions,all_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcStat2(model, threshold, loader, columns, label_index):\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "    all_scores = []\n",
    "    \n",
    "    probs1 =[]\n",
    "    labels1 = []\n",
    "    \n",
    "    for  index, (images, labels) in enumerate(loader):\n",
    "        \n",
    "#         if index % 1000 == 0:\n",
    "#             print(index)\n",
    "        \n",
    "        images = images.float().to(device)\n",
    "        labels = labels.float().to(device)\n",
    "        images = Variable(images)\n",
    "        labels = Variable(labels)\n",
    "        outputs = model(images)\n",
    "        probs = torch.sigmoid(outputs)\n",
    "        probs = probs.cpu().numpy()\n",
    "        \n",
    "        #for label_index, col in enumerate(columns):\n",
    "            \n",
    "        labels1.extend(labels[:, label_index].tolist())\n",
    "        probs1.extend(probs[:, label_index].tolist())\n",
    "            \n",
    "#         col_to_prob[col].append(my_probs)\n",
    "#         col_to_label[col].append(my_labels)\n",
    "        \n",
    "#         \n",
    "        \n",
    "        \n",
    "#         all_scores.extend(probs[:, label_index].tolist())\n",
    "#         results = np.zeros(shape=probs[:, label_index].shape)\n",
    "#         results[np.where(probs[:, label_index] >= threshold)] = 1\n",
    "#         all_predictions.extend(results.tolist())\n",
    "        \n",
    "#         all_labels.extend(labels[:,label_index].tolist())\n",
    "    \n",
    "    return probs1,labels1\n",
    "    #return all_labels, all_predictions,all_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Find_Optimal_Cutoff(target, predicted):\n",
    "    fpr, tpr, threshold = roc_curve(target, predicted)\n",
    "    i = np.arange(len(tpr)) \n",
    "    roc = pd.DataFrame({'tf' : pd.Series(tpr-(1-fpr), index=i), 'threshold' : pd.Series(threshold, index=i)})\n",
    "    roc_t = roc.ix[(roc.tf-0).abs().argsort()[:1]]\n",
    "    \n",
    "    \n",
    "    \n",
    "    optimal_idx = np.argmin(np.abs(tpr - fpr)) \n",
    "    optimal_threshold = thresholds[optimal_idx]\n",
    "\n",
    "    print('t1={0} ||| t2={1}'.format(optimal_threshold, roc_t['threshold']))\n",
    "    return list(roc_t['threshold']) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label = 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gabiburabia24/.local/lib/python3.5/site-packages/ipykernel_launcher.py:5: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t1=2.0 ||| t2=60931    0.476205\n",
      "Name: threshold, dtype: float64\n",
      "opt = [0.47620466351509094]\n",
      "label = 2\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-87-683f7f15b822>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabel_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mcol_to_prob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol_to_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalcStat2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-34-47450525dd48>\u001b[0m in \u001b[0;36mcalcStat2\u001b[0;34m(model, threshold, loader, columns, label_index)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mlabels1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0;32mfor\u001b[0m  \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m#         if index % 1000 == 0:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    558\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    561\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontainer_abcs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0mtransposed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msg_fmt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontainer_abcs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0mtransposed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msg_fmt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msg_fmt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# scalars\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mpy_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'float'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0mstorage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new_shared\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0melem_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__module__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'numpy'\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0melem_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'str_'\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0melem_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'string_'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_dataset_loader2 = DatasetLoader(X_train, y_train)\n",
    "train_loader2 = DataLoader(train_dataset_loader2, batch_size=500, shuffle=True, num_workers=0)\n",
    "from sklearn.metrics import roc_curve, f1_score, accuracy_score\n",
    "f1_thre = []\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    for label_index in range(Y.shape[1]):\n",
    "        label_index = 2\n",
    "        print('label = {0}'.format(label_index))\n",
    "        \n",
    "        label = Y.columns.tolist()[label_index]\n",
    "        col_to_prob, col_to_label = calcStat2(model, 0.5, train_loader2, Y.columns.tolist(), label_index)\n",
    "        \n",
    "        \n",
    "        f1_score(col_to_label, curent_predictions)\n",
    "        \n",
    "        opt1 = Find_Optimal_Cutoff(col_to_label, col_to_prob)\n",
    "        \n",
    "        print('opt = {0}'.format(opt1))\n",
    "        \n",
    "        f1_thre.append((label, opt1))\n",
    "        \n",
    "        \n",
    "#         fpr, tpr, thresholds = roc_curve(col_to_label, col_to_prob, pos_label=1,drop_intermediate=True)\n",
    "#         prev = None\n",
    "#         accuracy_scores = []\n",
    "#         f1_thre = []\n",
    "#         print(len(thresholds))\n",
    "#         for index, thresh in enumerate(thresholds):\n",
    "\n",
    "#             if index %1000 == 0:\n",
    "#                 print(index)\n",
    "\n",
    "#             if prev is not None:\n",
    "#                 if abs(prev - thresh ) <= 0.01:\n",
    "#                     accuracy_scores.append(-1)\n",
    "#                     continue \n",
    "\n",
    "#             accuracy_scores.append(f1_score(col_to_label,  [1 if m > thresh else 0 for m in col_to_prob]))\n",
    "\n",
    "#             prev = thresh\n",
    "#         accuracies = np.array(accuracy_scores)\n",
    "#         max_accuracy = accuracies.max() \n",
    "#         max_accuracy_threshold =  thresholds[accuracies.argmax()]\n",
    "#         index = accuracies.argmax()\n",
    "#         f1_thre.append((label, accuracies[index], thresholds[index]))\n",
    "\n",
    "#         print((label, accuracies[index], thresholds[index]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "# x = json.dumps(f1_thre)\n",
    "# obj = open('thresholds.txt', 'wb')\n",
    "# obj.write(x)\n",
    "# obj.close()\n",
    "\n",
    "import json\n",
    "with open('thresholds.json', 'w') as outfile:\n",
    "    json.dump(f1_thre, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "go_term_threhold = {p[0]: p[1] for p in f1_thre}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 0.5293810367584229)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thresh,threshold[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'go_10124': [0.6160830855369568],\n",
       " 'go_10125': [0.5106567740440369],\n",
       " 'go_10133': [0.6203318238258362],\n",
       " 'go_105': [0.5293810367584229],\n",
       " 'go_10501': [0.4540943205356598],\n",
       " 'go_1407': [0.6428552865982056],\n",
       " 'go_1514': [0.7291839718818665],\n",
       " 'go_15628': [0.6988471746444702],\n",
       " 'go_15671': [0.6800072193145752],\n",
       " 'go_15694': [0.8543277382850647],\n",
       " 'go_15700': [0.7183036208152771],\n",
       " 'go_15703': [0.6315870881080627],\n",
       " 'go_15707': [0.7267354130744934],\n",
       " 'go_15734': [0.7814751267433167],\n",
       " 'go_15753': [0.6684572696685791],\n",
       " 'go_15756': [0.7203608751296997],\n",
       " 'go_15757': [0.6328644156455994],\n",
       " 'go_15768': [0.6038869023323059],\n",
       " 'go_15794': [0.6414404511451721],\n",
       " 'go_15796': [0.031077101826667786],\n",
       " 'go_15871': [0.4894276559352875],\n",
       " 'go_15889': [0.7382705807685852],\n",
       " 'go_15941': [0.5093042254447937],\n",
       " 'go_15974': [0.1760285347700119],\n",
       " 'go_15976': [0.6299612522125244],\n",
       " 'go_15979': [0.6327002644538879],\n",
       " 'go_15990': [0.7340695858001709],\n",
       " 'go_15991': [0.5050241351127625],\n",
       " 'go_16036': [0.8674595355987549],\n",
       " 'go_162': [0.6415122747421265],\n",
       " 'go_16260': [0.8047489523887634],\n",
       " 'go_16539': [0.6429812908172607],\n",
       " 'go_16598': [0.4486657679080963],\n",
       " 'go_1680': [0.3040436804294586],\n",
       " 'go_17013': [0.7159241437911987],\n",
       " 'go_18160': [0.6701866984367371],\n",
       " 'go_18189': [0.6106153726577759],\n",
       " 'go_18339': [0.46070748567581177],\n",
       " 'go_1900753': [0.6929324269294739],\n",
       " 'go_1901023': [0.6818230152130127],\n",
       " 'go_1901106': [0.5905283689498901],\n",
       " 'go_1902604': [0.7743310928344727],\n",
       " 'go_19068': [0.8192129731178284],\n",
       " 'go_19242': [0.39010417461395264],\n",
       " 'go_19251': [0.6449214220046997],\n",
       " 'go_19277': [0.5752167105674744],\n",
       " 'go_19281': [0.5769836902618408],\n",
       " 'go_19284': [0.3424166142940521],\n",
       " 'go_19285': [0.8172468543052673],\n",
       " 'go_19295': [0.7828038334846497],\n",
       " 'go_19310': [0.509358286857605],\n",
       " 'go_19333': [0.5572584271430969],\n",
       " 'go_19354': [0.6649510860443115],\n",
       " 'go_19358': [0.6040951013565063],\n",
       " 'go_19379': [0.49620527029037476],\n",
       " 'go_19427': [0.11079917103052139],\n",
       " 'go_19428': [0.4504541754722595],\n",
       " 'go_19430': [0.6325446963310242],\n",
       " 'go_19441': [0.6243079900741577],\n",
       " 'go_19464': [0.472278356552124],\n",
       " 'go_19475': [0.660828709602356],\n",
       " 'go_19491': [0.743466317653656],\n",
       " 'go_19509': [0.6254629492759705],\n",
       " 'go_19512': [0.6790923476219177],\n",
       " 'go_19516': [0.7115200161933899],\n",
       " 'go_19544': [0.5288988947868347],\n",
       " 'go_19545': [0.5179653763771057],\n",
       " 'go_19551': [0.5277631282806396],\n",
       " 'go_19556': [0.6255269646644592],\n",
       " 'go_19569': [0.902837872505188],\n",
       " 'go_19629': [0.45895856618881226],\n",
       " 'go_19645': [0.8151966333389282],\n",
       " 'go_19646': [0.738463819026947],\n",
       " 'go_19684': [0.7685151696205139],\n",
       " 'go_19700': [0.6848157644271851],\n",
       " 'go_19746': [0.10277590155601501],\n",
       " 'go_2100': [0.6249285936355591],\n",
       " 'go_2143': [0.6926352977752686],\n",
       " 'go_2949': [0.7337005138397217],\n",
       " 'go_30153': [0.9195775389671326],\n",
       " 'go_30253': [0.6781408786773682],\n",
       " 'go_30254': [0.5847994685173035],\n",
       " 'go_30255': [0.6718880534172058],\n",
       " 'go_30259': [0.49684470891952515],\n",
       " 'go_30388': [0.7991524934768677],\n",
       " 'go_30420': [0.7437900304794312],\n",
       " 'go_30643': [0.7740371227264404],\n",
       " 'go_31388': [0.6241849064826965],\n",
       " 'go_31412': [0.7869919538497925],\n",
       " 'go_31460': [0.6563645005226135],\n",
       " 'go_31564': [0.6964530348777771],\n",
       " 'go_32049': [0.3638226091861725],\n",
       " 'go_32218': [0.8541174530982971],\n",
       " 'go_32775': [0.5703985691070557],\n",
       " 'go_32968': [0.45468869805336],\n",
       " 'go_33539': [0.5631517171859741],\n",
       " 'go_34194': [0.8133552670478821],\n",
       " 'go_34229': [0.4021284878253937],\n",
       " 'go_34258': [0.705571174621582],\n",
       " 'go_35429': [0.6691697239875793],\n",
       " 'go_35998': [0.2243044525384903],\n",
       " 'go_42121': [0.6526785492897034],\n",
       " 'go_42122': [0.6917805075645447],\n",
       " 'go_42128': [0.6213557124137878],\n",
       " 'go_42254': [0.6264568567276001],\n",
       " 'go_42351': [0.8759213089942932],\n",
       " 'go_42450': [0.6332072615623474],\n",
       " 'go_42619': [0.5262144207954407],\n",
       " 'go_42882': [0.5428441166877747],\n",
       " 'go_42906': [0.566574215888977],\n",
       " 'go_42916': [0.6343259811401367],\n",
       " 'go_42919': [0.7748274803161621],\n",
       " 'go_42953': [0.688430905342102],\n",
       " 'go_42956': [0.6684061884880066],\n",
       " 'go_43093': [0.5240561962127686],\n",
       " 'go_43165': [0.6707852482795715],\n",
       " 'go_43215': [0.6957799792289734],\n",
       " 'go_43419': [0.5578680038452148],\n",
       " 'go_43571': [0.7548685669898987],\n",
       " 'go_43953': [0.7456492185592651],\n",
       " 'go_44205': [0.5513159036636353],\n",
       " 'go_44209': [0.414826899766922],\n",
       " 'go_44341': [0.6496021151542664],\n",
       " 'go_44780': [0.5010768175125122],\n",
       " 'go_45150': [0.5668533444404602],\n",
       " 'go_45151': [0.4882948100566864],\n",
       " 'go_45227': [0.7195490002632141],\n",
       " 'go_45454': [0.5214142799377441],\n",
       " 'go_45982': [0.8566645979881287],\n",
       " 'go_46168': [0.6457592248916626],\n",
       " 'go_46294': [0.7199517488479614],\n",
       " 'go_46295': [0.8343303799629211],\n",
       " 'go_46306': [0.91501384973526],\n",
       " 'go_46355': [0.6368607878684998],\n",
       " 'go_46392': [0.5869906544685364],\n",
       " 'go_46411': [0.6840457320213318],\n",
       " 'go_46656': [0.5812634229660034],\n",
       " 'go_46854': [0.6062804460525513],\n",
       " 'go_48034': [0.6785887479782104],\n",
       " 'go_48473': [0.8143580555915833],\n",
       " 'go_50787': [0.8031458854675293],\n",
       " 'go_50821': [0.7366333603858948],\n",
       " 'go_50992': [0.47144830226898193],\n",
       " 'go_51085': [0.6860209703445435],\n",
       " 'go_51131': [0.9117456674575806],\n",
       " 'go_51365': [0.8380941152572632],\n",
       " 'go_51391': [0.6938081383705139],\n",
       " 'go_51692': [0.0178076159209013],\n",
       " 'go_52837': [0.25390011072158813],\n",
       " 'go_5978': [0.5563790202140808],\n",
       " 'go_5980': [0.7023181915283203],\n",
       " 'go_6015': [0.8842747807502747],\n",
       " 'go_6021': [0.5210123658180237],\n",
       " 'go_6069': [0.11364725977182388],\n",
       " 'go_6094': [0.44424399733543396],\n",
       " 'go_6097': [0.5103334784507751],\n",
       " 'go_6098': [0.5648617148399353],\n",
       " 'go_6102': [0.6525021195411682],\n",
       " 'go_6108': [0.5296156406402588],\n",
       " 'go_6120': [0.3363616466522217],\n",
       " 'go_6122': [0.7904208302497864],\n",
       " 'go_6146': [0.5955674648284912],\n",
       " 'go_6147': [0.409633606672287],\n",
       " 'go_6168': [0.7383818030357361],\n",
       " 'go_6171': [0.7281911373138428],\n",
       " 'go_6183': [0.6491495370864868],\n",
       " 'go_6189': [0.5124648213386536],\n",
       " 'go_6200': [0.6520378589630127],\n",
       " 'go_6203': [0.11976397782564163],\n",
       " 'go_6207': [0.4974595606327057],\n",
       " 'go_6212': [0.3279154896736145],\n",
       " 'go_6223': [0.7309722304344177],\n",
       " 'go_6228': [0.6497160196304321],\n",
       " 'go_6229': [0.6526087522506714],\n",
       " 'go_6233': [0.7317987084388733],\n",
       " 'go_6265': [0.5032874345779419],\n",
       " 'go_6284': [0.629278302192688],\n",
       " 'go_6303': [0.47075313329696655],\n",
       " 'go_6367': [0.7159028053283691],\n",
       " 'go_6388': [0.7300958633422852],\n",
       " 'go_6535': [0.5656159520149231],\n",
       " 'go_6556': [0.529240608215332],\n",
       " 'go_6564': [0.5768809914588928],\n",
       " 'go_6614': [0.43818163871765137],\n",
       " 'go_6729': [0.7862727642059326],\n",
       " 'go_6741': [0.5862220525741577],\n",
       " 'go_6742': [0.779079020023346],\n",
       " 'go_6750': [0.612723171710968],\n",
       " 'go_6784': [0.7400694489479065],\n",
       " 'go_6788': [0.8371418118476868],\n",
       " 'go_6798': [0.30439281463623047],\n",
       " 'go_6799': [0.20332171022891998],\n",
       " 'go_6809': [0.5423859357833862],\n",
       " 'go_6824': [0.7190190553665161],\n",
       " 'go_6878': [0.8589180111885071],\n",
       " 'go_70329': [0.7406940460205078],\n",
       " 'go_70395': [0.6804408431053162],\n",
       " 'go_70409': [0.483561247587204],\n",
       " 'go_70526': [0.6138315200805664],\n",
       " 'go_7156': [0.43766143918037415],\n",
       " 'go_71586': [0.7835248112678528],\n",
       " 'go_71722': [0.5882266163825989],\n",
       " 'go_71766': [0.8497763276100159],\n",
       " 'go_71918': [0.4192432463169098],\n",
       " 'go_71945': [0.9521514177322388],\n",
       " 'go_71951': [0.6414999961853027],\n",
       " 'go_7205': [0.6498671174049377],\n",
       " 'go_72488': [0.6551252603530884],\n",
       " 'go_8218': [0.6843069791793823],\n",
       " 'go_8295': [0.5861122012138367],\n",
       " 'go_8612': [0.635798454284668],\n",
       " 'go_8615': [0.46052390336990356],\n",
       " 'go_90124': [0.8041936755180359],\n",
       " 'go_9052': [0.7561582326889038],\n",
       " 'go_9088': [0.5736076235771179],\n",
       " 'go_9102': [0.624413013458252],\n",
       " 'go_9103': [0.5650036334991455],\n",
       " 'go_9107': [0.6204837560653687],\n",
       " 'go_917': [0.6385888457298279],\n",
       " 'go_918': [0.8653765320777893],\n",
       " 'go_921': [0.37260106205940247],\n",
       " 'go_9229': [0.6210270524024963],\n",
       " 'go_9231': [0.6393592953681946],\n",
       " 'go_9234': [0.5750722885131836],\n",
       " 'go_9239': [0.6898089051246643],\n",
       " 'go_9244': [0.5714091062545776],\n",
       " 'go_9245': [0.47839269042015076],\n",
       " 'go_9246': [0.7430661916732788],\n",
       " 'go_9254': [0.6707454919815063],\n",
       " 'go_9255': [0.4034596085548401],\n",
       " 'go_9271': [0.32599112391471863],\n",
       " 'go_9294': [0.7300188541412354],\n",
       " 'go_9298': [0.5630684494972229],\n",
       " 'go_9307': [0.548001229763031],\n",
       " 'go_9399': [0.6041181087493896],\n",
       " 'go_9401': [0.47620466351509094],\n",
       " 'go_9405': [0.6016863584518433],\n",
       " 'go_9432': [0.5024814009666443],\n",
       " 'go_9436': [0.8037524223327637],\n",
       " 'go_9447': [0.8011929392814636],\n",
       " 'go_9450': [0.6898192167282104],\n",
       " 'go_9758': [0.7680894136428833],\n",
       " 'go_9772': [0.7062844634056091],\n",
       " 'go_9877': [0.7110932469367981],\n",
       " 'go_9972': [0.6380934715270996]}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "go_term_threhold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_on_sums1 = Y.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "go_9401     168838\n",
       "go_45454    134060\n",
       "go_105       83332\n",
       "dtype: int64"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_on_sums1[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "labels_on_sums1.sort_values(ascending=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('go_9401', 168838)\n",
      "('go_45454', 134060)\n",
      "('go_105', 83332)\n"
     ]
    }
   ],
   "source": [
    "tt =1\n",
    "for x in labels_on_sums1.items():\n",
    "    print(x)\n",
    "    tt += 1\n",
    "    if tt > 3:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_loader3 = DatasetLoader(X_train, y_train)\n",
    "train_loader3 = DataLoader(train_dataset_loader3, batch_size=800, shuffle=True, num_workers=0)\n",
    "all_probs = []\n",
    "all_labels1 = []\n",
    "with torch.no_grad():\n",
    "    for  index, (images, labels) in enumerate(train_loader3):\n",
    "        images = images.float().to(device)\n",
    "        labels = labels.float().to(device)\n",
    "        images = Variable(images)\n",
    "        labels = Variable(labels)\n",
    "        outputs = model(images)\n",
    "        probs = torch.sigmoid(outputs)\n",
    "        probs = probs.cpu().numpy()\n",
    "\n",
    "        all_probs.append(probs)\n",
    "        #labels[:, label_index].tolist()\n",
    "        all_labels1.append(labels)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2213, 245)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_labels1), len(Y.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "column=go_105 best thre=0.95 f1=0.44708334168286473 f11=0.511468210028752 f12=0.6967883368749496 f13=0.8006922411306605 f14=0.8118651785631312 f15=0.7860983448980206 max_index = 4 \n",
      "\n",
      "\n",
      "1\n",
      "column=go_45454 best thre=0.95 f1=0.5278235003644743 f11=0.582707855995365 f12=0.7163487314297574 f13=0.7734410427849784 f14=0.774737207888107 f15=0.7474738582845502 max_index = 4 \n",
      "\n",
      "\n",
      "2\n",
      "column=go_9401 best thre=0.9 f1=0.6697157829785616 f11=0.7133668569414637 f12=0.8060323459578735 f13=0.8408949946626049 f14=0.8326428975664969 f15=0.7886319789248954 max_index = 3 \n",
      "\n",
      "\n",
      "3\n",
      "column=go_45227 best thre=0.98 f1=0.1904191153727188 f11=0.2246597975996512 f12=0.3379428327036852 f13=0.47351407716371224 f14=0.5572572893303428 f15=0.6184129901960785 max_index = 5 \n",
      "\n",
      "\n",
      "4\n",
      "column=go_42128 best thre=0.98 f1=0.2591531262384538 f11=0.2962158183873201 f12=0.4294265502319194 f13=0.597952940287797 f14=0.6971576705860567 f15=0.7657577112203845 max_index = 5 \n",
      "\n",
      "\n",
      "5\n",
      "column=go_6189 best thre=0.98 f1=0.3758740747242459 f11=0.4317395842148317 f12=0.6048229026879753 f13=0.7568264058154642 f14=0.8104436992758906 f15=0.8173061312841603 max_index = 5 \n",
      "\n",
      "\n",
      "6\n",
      "column=go_19354 best thre=0.98 f1=0.13332808729409298 f11=0.16130534633112242 f12=0.27781346726625006 f13=0.4478910728910729 f14=0.573950177935943 f15=0.6730346737988346 max_index = 5 \n",
      "\n",
      "\n",
      "7\n",
      "column=go_71722 best thre=0.98 f1=0.10491487445814922 f11=0.11934432515337424 f12=0.17339887440336255 f13=0.2532708242477104 f14=0.3240174672489083 f15=0.42775763852925947 max_index = 5 \n",
      "\n",
      "\n",
      "8\n",
      "column=go_9231 best thre=0.95 f1=0.35857796470487935 f11=0.4112780637492837 f12=0.571394099403127 f13=0.697827414367035 f14=0.7212320470669665 f15=0.6843146891229562 max_index = 4 \n",
      "\n",
      "\n",
      "9\n",
      "column=go_19544 best thre=0.98 f1=0.2843292626083714 f11=0.3104818172557808 f12=0.38942369400590426 f13=0.487655317088914 f14=0.5558845299777941 f15=0.6549499443826474 max_index = 5 \n",
      "\n",
      "\n",
      "10\n",
      "column=go_9088 best thre=0.98 f1=0.2802322645796516 f11=0.3157185720409038 f12=0.44272799087158776 f13=0.5957437982245084 f14=0.6901065449010654 f15=0.7719559023906849 max_index = 5 \n",
      "\n",
      "\n",
      "11\n",
      "column=go_9245 best thre=0.98 f1=0.3443293881405975 f11=0.3980908152734778 f12=0.5599443389525867 f13=0.7035685162893643 f14=0.7633306357341966 f15=0.7891928123952666 max_index = 5 \n",
      "\n",
      "\n",
      "12\n",
      "column=go_8295 best thre=0.98 f1=0.19625785547702548 f11=0.23187304802903688 f12=0.3662497033797756 f13=0.5595078898101097 f14=0.6898661048052741 f15=0.8050251256281408 max_index = 5 \n",
      "\n",
      "\n",
      "13\n",
      "column=go_30259 best thre=0.98 f1=0.2151768488745981 f11=0.25684145137494596 f12=0.3963809625782966 f13=0.5373428897891347 f14=0.6164130642388744 f15=0.6720031391014323 max_index = 5 \n",
      "\n",
      "\n",
      "14\n",
      "column=go_6094 best thre=0.95 f1=0.3588909463990791 f11=0.424042114078904 f12=0.6278482186319279 f13=0.7977675122581865 f14=0.8324485028668507 f15=0.8184318326057272 max_index = 4 \n",
      "\n",
      "\n",
      "15\n",
      "column=go_6741 best thre=0.98 f1=0.2646969865804932 f11=0.3123439037311738 f12=0.4653731517737128 f13=0.624643037124139 f14=0.6950654582074521 f15=0.7308994197292069 max_index = 5 \n",
      "\n",
      "\n",
      "16\n",
      "column=go_90124 best thre=0.98 f1=0.047669809120890104 f11=0.053283049866512326 f12=0.07309841786267154 f13=0.10191956124314443 f14=0.1300275482093664 f15=0.1779928157759695 max_index = 5 \n",
      "\n",
      "\n",
      "17\n",
      "column=go_19310 best thre=0.98 f1=0.23603368064900612 f11=0.2727475269583816 f12=0.4067179279643869 f13=0.5782753879395011 f14=0.6844220502669306 f15=0.7767495662232504 max_index = 5 \n",
      "\n",
      "\n",
      "18\n",
      "column=go_6535 best thre=0.98 f1=0.3526548630828295 f11=0.4023600860177871 f12=0.5605801003549138 f13=0.7135167464114833 f14=0.7985616454043654 f15=0.8584564521128215 max_index = 5 \n",
      "\n",
      "\n",
      "19\n",
      "column=go_19700 best thre=0.98 f1=0.2034208918753818 f11=0.23722410021873136 f12=0.35962476084675676 f13=0.5180418294644908 f14=0.6323640813472409 f15=0.7310857059637369 max_index = 5 \n",
      "\n",
      "\n",
      "20\n",
      "column=go_50821 best thre=0.98 f1=0.44284284284284275 f11=0.47625201938610656 f12=0.576164991515468 f13=0.6989828353464717 f14=0.7928286852589642 f15=0.8808121827411167 max_index = 5 \n",
      "\n",
      "\n",
      "21\n",
      "column=go_19684 best thre=0.98 f1=0.10669496430638627 f11=0.12107918403158588 f12=0.16968600916716037 f13=0.24462353212065394 f14=0.31851393188854493 f15=0.4356979405034325 max_index = 5 \n",
      "\n",
      "\n",
      "22\n",
      "column=go_6098 best thre=0.95 f1=0.38405554336148234 f11=0.4493497065289601 f12=0.6376043079705677 f13=0.7656144883076342 f14=0.7945638144408169 f15=0.7711494000708776 max_index = 4 \n",
      "\n",
      "\n",
      "23\n",
      "column=go_6614 best thre=0.98 f1=0.3777217615457439 f11=0.4331290602561742 f12=0.5996171074887083 f13=0.7476081132797551 f14=0.8162378256553353 f15=0.8612708153355776 max_index = 5 \n",
      "\n",
      "\n",
      "24\n",
      "column=go_15703 best thre=0.98 f1=0.2586703443799943 f11=0.28028869427452363 f12=0.35839954597048806 f13=0.46503588471799495 f14=0.5535689663096934 f15=0.6493717065261452 max_index = 5 \n",
      "\n",
      "\n",
      "25\n",
      "column=go_6564 best thre=0.98 f1=0.24297979359986122 f11=0.2883032326290252 f12=0.44805999240184347 f13=0.6375716261608378 f14=0.7424485715151331 f15=0.7937275539593927 max_index = 5 \n",
      "\n",
      "\n",
      "26\n",
      "column=go_9432 best thre=0.98 f1=0.4359766789409083 f11=0.5099910989563848 f12=0.7127720345698122 f13=0.8513147194361615 f14=0.8980692167577414 f15=0.9152161132837461 max_index = 5 \n",
      "\n",
      "\n",
      "27\n",
      "column=go_15991 best thre=0.98 f1=0.4216299838267351 f11=0.48069321982365465 f12=0.6577941560947673 f13=0.8101541422778978 f14=0.8698032924107143 f15=0.8909684357489199 max_index = 5 \n",
      "\n",
      "\n",
      "28\n",
      "column=go_48034 best thre=0.98 f1=0.48504897748754083 f11=0.5051689415976729 f12=0.5746329526916802 f13=0.6433526342945595 f14=0.6917358443811176 f15=0.7775099262620534 max_index = 5 \n",
      "\n",
      "\n",
      "29\n",
      "column=go_15976 best thre=0.98 f1=0.22400909681611433 f11=0.262440701709701 f12=0.40456972556811127 f13=0.5732044198895027 f14=0.6730065184391463 f15=0.7641823641823643 max_index = 5 \n",
      "\n",
      "\n",
      "30\n",
      "column=go_6284 best thre=0.98 f1=0.4198541449620479 f11=0.46226165546125864 f12=0.5925212339596279 f13=0.7120111358999295 f14=0.764084974551892 f15=0.769466937866984 max_index = 5 \n",
      "\n",
      "\n",
      "31\n",
      "column=go_6207 best thre=0.98 f1=0.36154488711511246 f11=0.4195564269084671 f12=0.6016205128205129 f13=0.7550309973232735 f14=0.7986990649529349 f15=0.8008161445741817 max_index = 5 \n",
      "\n",
      "\n",
      "32\n",
      "column=go_15628 best thre=0.98 f1=0.49060583933145563 f11=0.5375982471560872 f12=0.664498230173156 f13=0.7789173898820404 f14=0.8229646859801225 f15=0.8377083822493543 max_index = 5 \n",
      "\n",
      "\n",
      "33\n",
      "column=go_10124 best thre=0.98 f1=0.22609440363648342 f11=0.2695420974889217 f12=0.43487252399263254 f13=0.6337878348322213 f14=0.742245242390735 f15=0.8203157172271791 max_index = 5 \n",
      "\n",
      "\n",
      "34\n",
      "column=go_6750 best thre=0.98 f1=0.15366620279284035 f11=0.17952795979388 f12=0.28949368291061084 f13=0.47032284768211924 f14=0.6066260909508497 f15=0.7199162888036276 max_index = 5 \n",
      "\n",
      "\n",
      "35\n",
      "column=go_6265 best thre=0.98 f1=0.529233299623262 f11=0.595484175720359 f12=0.7633139539719374 f13=0.8600989653621233 f14=0.8884800864721045 f15=0.89491975987885 max_index = 5 \n",
      "\n",
      "\n",
      "36\n",
      "column=go_44780 best thre=0.98 f1=0.5106587072516894 f11=0.5629343116900354 f12=0.691941891491254 f13=0.7919551322396118 f14=0.834819326518523 f15=0.8564428879133165 max_index = 5 \n",
      "\n",
      "\n",
      "37\n",
      "column=go_19428 best thre=0.98 f1=0.09722376252264242 f11=0.11292055171228461 f12=0.17552583831731738 f13=0.2787206266318538 f14=0.37934158864391426 f15=0.521117608836907 max_index = 5 \n",
      "\n",
      "\n",
      "38\n",
      "column=go_31564 best thre=0.98 f1=0.36901706613307816 f11=0.4166215173455483 f12=0.5803936130709245 f13=0.7401608513501117 f14=0.8139973458126702 f15=0.8626909263132968 max_index = 5 \n",
      "\n",
      "\n",
      "39\n",
      "column=go_6183 best thre=0.98 f1=0.2783718597255128 f11=0.32989220038046924 f12=0.5267328350283871 f13=0.7495267392333176 f14=0.8563763929013619 f15=0.9124812030075188 max_index = 5 \n",
      "\n",
      "\n",
      "40\n",
      "column=go_6223 best thre=0.98 f1=0.49429331502617346 f11=0.5441398865784499 f12=0.6873239436619719 f13=0.8143746868065 f14=0.8807368092413363 f15=0.9272043460091934 max_index = 5 \n",
      "\n",
      "\n",
      "41\n",
      "column=go_30255 best thre=0.98 f1=0.16276292378561505 f11=0.18370108530557014 f12=0.2638973237782906 f13=0.4225198141568735 f14=0.5814068856037305 f15=0.7291306850892343 max_index = 5 \n",
      "\n",
      "\n",
      "42\n",
      "column=go_9107 best thre=0.98 f1=0.2595584156218795 f11=0.3022682445759369 f12=0.4528264489019265 f13=0.6334560515416475 f14=0.7309791997788682 f15=0.7986378928366292 max_index = 5 \n",
      "\n",
      "\n",
      "43\n",
      "column=go_15700 best thre=0.98 f1=0.27208071452199795 f11=0.29895375284306297 f12=0.393662793035506 f13=0.5460108794197643 f14=0.6722269746221842 f15=0.8022509452211378 max_index = 5 \n",
      "\n",
      "\n",
      "44\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "column=go_1407 best thre=0.98 f1=0.4928767519144632 f11=0.5473027310451315 f12=0.6883915395966552 f13=0.798472101344164 f14=0.8447765841507083 f15=0.8731339172588553 max_index = 5 \n",
      "\n",
      "\n",
      "45\n",
      "column=go_6108 best thre=0.98 f1=0.3693691423964527 f11=0.4237725349255116 f12=0.5937107436314587 f13=0.7546184090052244 f14=0.8290300921687115 f15=0.87024463475383 max_index = 5 \n",
      "\n",
      "\n",
      "46\n",
      "column=go_43165 best thre=0.98 f1=0.4400554924959011 f11=0.49802552509586223 f12=0.6594910432843852 f13=0.7752814237003129 f14=0.8271858625295386 f15=0.8619330483539666 max_index = 5 \n",
      "\n",
      "\n",
      "47\n",
      "column=go_7205 best thre=0.98 f1=0.13697166592170246 f11=0.16023036972283644 f12=0.24155115711316907 f13=0.3551646110208115 f14=0.44468452895419186 f15=0.5385231261932623 max_index = 5 \n",
      "\n",
      "\n",
      "48\n",
      "column=go_42450 best thre=0.98 f1=0.3182048798487155 f11=0.365682362330407 f12=0.5214118370011906 f13=0.6824950110013815 f14=0.7727647543880316 f15=0.8486587481649539 max_index = 5 \n",
      "\n",
      "\n",
      "49\n",
      "column=go_16539 best thre=0.98 f1=0.041316482689412945 f11=0.04759169606958234 f12=0.07225233537077616 f13=0.11276699029126215 f14=0.1498285595539085 f15=0.20672314840120254 max_index = 5 \n",
      "\n",
      "\n",
      "50\n",
      "column=go_50992 best thre=0.98 f1=0.2699391968640727 f11=0.31527556588256794 f12=0.4743145915403287 f13=0.6542576905929558 f14=0.7522193211488251 f15=0.8302222222222223 max_index = 5 \n",
      "\n",
      "\n",
      "51\n",
      "column=go_19277 best thre=0.98 f1=0.3781015364725772 f11=0.43462785958265165 f12=0.6037164820531444 f13=0.739660228536758 f14=0.801076927391536 f15=0.8419520653991454 max_index = 5 \n",
      "\n",
      "\n",
      "52\n",
      "column=go_9234 best thre=0.98 f1=0.26760868622946554 f11=0.3113344256326234 f12=0.46651037795185546 f13=0.6338934756861601 f14=0.7047108710786693 f15=0.7114032638765514 max_index = 5 \n",
      "\n",
      "\n",
      "53\n",
      "column=go_5980 best thre=0.98 f1=0.2716239123064753 f11=0.3036980749746707 f12=0.4078017850373893 f13=0.5212756427483651 f14=0.5939236561931966 f15=0.6736491898219457 max_index = 5 \n",
      "\n",
      "\n",
      "54\n",
      "column=go_19430 best thre=0.98 f1=0.4345756379741496 f11=0.4820018734109461 f12=0.6154966376836425 f13=0.7436044391184286 f14=0.8159244109005014 f15=0.8811743951612904 max_index = 5 \n",
      "\n",
      "\n",
      "55\n",
      "column=go_6798 best thre=0.98 f1=0.24446297840793255 f11=0.2616544336441182 f12=0.30951821386603995 f13=0.36368676368676367 f14=0.40339244410177333 f15=0.45151089248067466 max_index = 5 \n",
      "\n",
      "\n",
      "56\n",
      "column=go_162 best thre=0.98 f1=0.365667603840645 f11=0.4134332674603557 f12=0.5776334899615548 f13=0.7394999999999999 f14=0.8121343893979396 f15=0.855893172667497 max_index = 5 \n",
      "\n",
      "\n",
      "57\n",
      "column=go_6824 best thre=0.98 f1=0.24653260714146444 f11=0.2826086956521739 f12=0.40698083347986636 f13=0.5590506329113925 f14=0.6630426156654233 f15=0.7411549309713493 max_index = 5 \n",
      "\n",
      "\n",
      "58\n",
      "column=go_30643 best thre=0.98 f1=0.669384563833352 f11=0.7025632001876944 f12=0.7867385870280226 f13=0.8518254420992584 f14=0.891163068648689 f15=0.9259894047990027 max_index = 5 \n",
      "\n",
      "\n",
      "59\n",
      "column=go_19509 best thre=0.98 f1=0.1698563838181914 f11=0.2035937262067915 f12=0.335171700633375 f13=0.5150713539853811 f14=0.6366559485530547 f15=0.7409725844780618 max_index = 5 \n",
      "\n",
      "\n",
      "60\n",
      "column=go_70526 best thre=0.98 f1=0.3139156210663094 f11=0.3569117482985607 f12=0.49397284777936484 f13=0.6362765330188679 f14=0.6960204326660699 f15=0.7088548307356042 max_index = 5 \n",
      "\n",
      "\n",
      "61\n",
      "column=go_8615 best thre=0.98 f1=0.29119964034290036 f11=0.3477800878369235 f12=0.5291621663323561 f13=0.6958257906506228 f14=0.7819689506827109 f15=0.8289445965873182 max_index = 5 \n",
      "\n",
      "\n",
      "62\n",
      "column=go_19464 best thre=0.98 f1=0.49160389745007255 f11=0.5516545757846142 f12=0.7212047212047212 f13=0.8601570166830227 f14=0.9122876074263596 f15=0.9439007580978636 max_index = 5 \n",
      "\n",
      "\n",
      "63\n",
      "column=go_19068 best thre=0.98 f1=0.0820946098585822 f11=0.09289182315616411 f12=0.13497165899494046 f13=0.20900944538629207 f14=0.2802979011509817 f15=0.3902929712058343 max_index = 5 \n",
      "\n",
      "\n",
      "64\n",
      "column=go_32775 best thre=0.98 f1=0.18361173767969816 f11=0.21268493815183118 f12=0.32254018926006156 f13=0.48003599820009 f14=0.5865842055185538 f15=0.6705113810350107 max_index = 5 \n",
      "\n",
      "\n",
      "65\n",
      "column=go_16598 best thre=0.98 f1=0.2618487181118392 f11=0.2881557750227639 f12=0.38176585275276204 f13=0.49709302325581395 f14=0.5761421319796954 f15=0.6840511440107672 max_index = 5 \n",
      "\n",
      "\n",
      "66\n",
      "column=go_46854 best thre=0.98 f1=0.27857533626365333 f11=0.3252803375152659 f12=0.47726306119324274 f13=0.6317922179368257 f14=0.6994672143346131 f15=0.706658539209007 max_index = 5 \n",
      "\n",
      "\n",
      "67\n",
      "column=go_46295 best thre=0.98 f1=0.313345521023766 f11=0.35422173845436167 f12=0.4680289129830414 f13=0.5941871026339691 f14=0.6699809604400253 f15=0.7477522477522477 max_index = 5 \n",
      "\n",
      "\n",
      "68\n",
      "column=go_15753 best thre=0.98 f1=0.3375779397558619 f11=0.36634230108757865 f12=0.46589665653495443 f13=0.569640995084165 f14=0.6397046484309448 f15=0.7087942996437278 max_index = 5 \n",
      "\n",
      "\n",
      "69\n",
      "column=go_6233 best thre=0.98 f1=0.24997174822013787 f11=0.28859839278285054 f12=0.4175528423263752 f13=0.579158413449925 f14=0.6724859500179361 f15=0.7497307001795332 max_index = 5 \n",
      "\n",
      "\n",
      "70\n",
      "column=go_5978 best thre=0.98 f1=0.31639102496576427 f11=0.3632476040276598 f12=0.5098893514224705 f13=0.6431961583561338 f14=0.71648987463838 f15=0.7800571280186965 max_index = 5 \n",
      "\n",
      "\n",
      "71\n",
      "column=go_2100 best thre=0.98 f1=0.30271362728908385 f11=0.3495195460636994 f12=0.5076762183643589 f13=0.6700821979506811 f14=0.7594854045582186 f15=0.8271031482710314 max_index = 5 \n",
      "\n",
      "\n",
      "72\n",
      "column=go_1514 best thre=0.98 f1=0.22604195545698177 f11=0.25532806931730834 f12=0.3512204666634055 f13=0.46468945338475864 f14=0.5393490888835839 f15=0.6244596022672687 max_index = 5 \n",
      "\n",
      "\n",
      "73\n",
      "column=go_2143 best thre=0.98 f1=0.2000125164278115 f11=0.22042564953012717 f12=0.29389949935101056 f13=0.3988307066598882 f14=0.4953047907050772 f15=0.6420633256447893 max_index = 5 \n",
      "\n",
      "\n",
      "74\n",
      "column=go_9405 best thre=0.95 f1=0.27245220385461927 f11=0.31145123836699123 f12=0.4420716008563918 f13=0.5774546616601423 f14=0.6217989835693492 f15=0.6112396023534186 max_index = 4 \n",
      "\n",
      "\n",
      "75\n",
      "column=go_19556 best thre=0.98 f1=0.13420641241188094 f11=0.15695244287571986 f12=0.24830321055441168 f13=0.398547586528532 f14=0.5237011755783088 f15=0.6184393407902573 max_index = 5 \n",
      "\n",
      "\n",
      "76\n",
      "column=go_16036 best thre=0.98 f1=0.5278181577904058 f11=0.5486263736263736 f12=0.6222915042868277 f13=0.701179785173446 f14=0.7555977229601518 f15=0.8300229981183358 max_index = 5 \n",
      "\n",
      "\n",
      "77\n",
      "column=go_42953 best thre=0.98 f1=0.5616902317079727 f11=0.608581512703089 f12=0.7285242677530379 f13=0.8327037894480321 f14=0.8829323570432357 f15=0.9117402164862616 max_index = 5 \n",
      "\n",
      "\n",
      "78\n",
      "column=go_9255 best thre=0.98 f1=0.5411214953271027 f11=0.5689019896831246 f12=0.6477177261271353 f13=0.7216981132075471 f14=0.774455144450076 f15=0.8309319715484224 max_index = 5 \n",
      "\n",
      "\n",
      "79\n",
      "column=go_9103 best thre=0.95 f1=0.330258658060787 f11=0.3914069785514454 f12=0.5848278051103 f13=0.7178487050327642 f14=0.7312000611690944 f15=0.6903132722883305 max_index = 4 \n",
      "\n",
      "\n",
      "80\n",
      "column=go_15694 best thre=0.98 f1=0.1647282935632645 f11=0.17976790015327349 f12=0.23146370939077276 f13=0.31140266255064636 f14=0.3859819907520078 f15=0.5142191917512058 max_index = 5 \n",
      "\n",
      "\n",
      "81\n",
      "column=go_6303 best thre=0.98 f1=0.3026329062846767 f11=0.34731582616778994 f12=0.49320550229262194 f13=0.6420517355737343 f14=0.7323799795709909 f15=0.8025851938895417 max_index = 5 \n",
      "\n",
      "\n",
      "82\n",
      "column=go_6729 best thre=0.98 f1=0.16824549699799865 f11=0.19389041871477466 f12=0.2913782991202346 f13=0.4331978967495219 f14=0.5525017618040874 f15=0.6925956738768719 max_index = 5 \n",
      "\n",
      "\n",
      "83\n",
      "column=go_15794 best thre=0.98 f1=0.5070305634215408 f11=0.559016260820322 f12=0.7080946794562142 f13=0.8163612088845734 f14=0.8551130247578039 f15=0.8845498503338707 max_index = 5 \n",
      "\n",
      "\n",
      "84\n",
      "column=go_15768 best thre=0.98 f1=0.31950796404352627 f11=0.3543301569506726 f12=0.46064433461468063 f13=0.5870564658729224 f14=0.6723526200873363 f15=0.7608764687202286 max_index = 5 \n",
      "\n",
      "\n",
      "85\n",
      "column=go_17013 best thre=0.98 f1=0.13009441571381125 f11=0.15218396603186615 f12=0.24138982017677538 f13=0.37720300261096606 f14=0.484285966045938 f15=0.5680343752632909 max_index = 5 \n",
      "\n",
      "\n",
      "86\n",
      "column=go_15979 best thre=0.98 f1=0.15094527183342973 f11=0.17706742133106076 f12=0.28132456076928636 f13=0.44307455210942015 f14=0.5403927417350236 f15=0.5834182491894396 max_index = 5 \n",
      "\n",
      "\n",
      "87\n",
      "column=go_71945 best thre=0.98 f1=0.1995703031749821 f11=0.2186064820889647 f12=0.2864069264069264 f13=0.37267362687244665 f14=0.44389302453818574 f15=0.5353846153846155 max_index = 5 \n",
      "\n",
      "\n",
      "88\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "column=go_46355 best thre=0.98 f1=0.13056594666344876 f11=0.14402998393717653 f12=0.18572106261859583 f13=0.24510830179563578 f14=0.29930866205774703 f15=0.37431192660550455 max_index = 5 \n",
      "\n",
      "\n",
      "89\n",
      "column=go_42619 best thre=0.98 f1=0.32372062010335056 f11=0.3751358063014124 f12=0.5299469718113313 f13=0.6711655541700287 f14=0.7563438852584231 f15=0.828955164921983 max_index = 5 \n",
      "\n",
      "\n",
      "90\n",
      "column=go_32049 best thre=0.98 f1=0.5493084995482661 f11=0.5981202152656713 f12=0.7259279804003143 f13=0.8257378356819995 f14=0.8747797419428182 f15=0.9128470758296693 max_index = 5 \n",
      "\n",
      "\n",
      "91\n",
      "column=go_6799 best thre=0.98 f1=0.397397476340694 f11=0.425433037600338 f12=0.5088289019687436 f13=0.5965457479232653 f14=0.6607368071689347 f15=0.7333184888295108 max_index = 5 \n",
      "\n",
      "\n",
      "92\n",
      "column=go_30388 best thre=0.98 f1=0.40149666503985676 f11=0.4411659513590844 f12=0.5632802657198488 f13=0.692329465043872 f14=0.7728068778856869 f15=0.8457993619283942 max_index = 5 \n",
      "\n",
      "\n",
      "93\n",
      "column=go_42254 best thre=0.98 f1=0.3524079838857352 f11=0.40377368434922395 f12=0.5614689855937897 f13=0.7179284443565922 f14=0.7974254742547425 f15=0.8598953681255582 max_index = 5 \n",
      "\n",
      "\n",
      "94\n",
      "column=go_9102 best thre=0.98 f1=0.31660650083841635 f11=0.3673612536782317 f12=0.5233988120116013 f13=0.6666896077081899 f14=0.7335255105126293 f15=0.7724285780864054 max_index = 5 \n",
      "\n",
      "\n",
      "95\n",
      "column=go_19441 best thre=0.98 f1=0.14996103540512648 f11=0.17550736862998068 f12=0.27794221282593373 f13=0.42938881664499345 f14=0.5425146602276647 f15=0.6556272803817008 max_index = 5 \n",
      "\n",
      "\n",
      "96\n",
      "column=go_6147 best thre=0.98 f1=0.1664250734573947 f11=0.19111733610445938 f12=0.2781289506953224 f13=0.39540989983467856 f14=0.49016493585827736 f15=0.6161680458306811 max_index = 5 \n",
      "\n",
      "\n",
      "97\n",
      "column=go_72488 best thre=0.98 f1=0.3589445542252702 f11=0.3921488760469439 f12=0.4928453761426403 f13=0.6289790980015854 f14=0.7280560043080238 f15=0.8515047441643867 max_index = 5 \n",
      "\n",
      "\n",
      "98\n",
      "column=go_9972 best thre=0.98 f1=0.2594694156442174 f11=0.29982460363861474 f12=0.43359375000000006 f13=0.5929215822345593 f14=0.6995418575593503 f15=0.797339593114241 max_index = 5 \n",
      "\n",
      "\n",
      "99\n",
      "column=go_9877 best thre=0.98 f1=0.07555386072601578 f11=0.0901787032934574 f12=0.14738689058779297 f13=0.23975720789074356 f14=0.3146689638198375 f15=0.42435597189695545 max_index = 5 \n",
      "\n",
      "\n",
      "100\n",
      "column=go_15941 best thre=0.98 f1=0.32082829097754473 f11=0.3610942617978891 f12=0.4866429575717476 f13=0.62251012145749 f14=0.7027834858957596 f15=0.7782162588792423 max_index = 5 \n",
      "\n",
      "\n",
      "101\n",
      "column=go_9307 best thre=0.98 f1=0.11858699860389268 f11=0.13954240780987348 f12=0.22552679404685888 f13=0.36962844811409273 f14=0.4739757930766788 f15=0.5351139527688976 max_index = 5 \n",
      "\n",
      "\n",
      "102\n",
      "column=go_19242 best thre=0.98 f1=0.2045051553162146 f11=0.24616968209351126 f12=0.4002588159171789 f13=0.613682092555332 f14=0.7403110343125154 f15=0.8530318602261048 max_index = 5 \n",
      "\n",
      "\n",
      "103\n",
      "column=go_42916 best thre=0.98 f1=0.3044330149399951 f11=0.3430858404637041 f12=0.46436609152288066 f13=0.6136590229312063 f14=0.7082971957213067 f15=0.8207547169811321 max_index = 5 \n",
      "\n",
      "\n",
      "104\n",
      "column=go_44341 best thre=0.98 f1=0.20986675912625685 f11=0.22991937891908032 f12=0.3021763292818471 f13=0.41029664133366023 f14=0.5031835546661816 f15=0.6233544989933405 max_index = 5 \n",
      "\n",
      "\n",
      "105\n",
      "column=go_9399 best thre=0.98 f1=0.19458275243975307 f11=0.2301838470685082 f12=0.36762041083583313 f13=0.5757915097220808 f14=0.7000721926888493 f15=0.7790193661271723 max_index = 5 \n",
      "\n",
      "\n",
      "106\n",
      "column=go_9254 best thre=0.98 f1=0.19269408177624892 f11=0.22569739898351082 f12=0.3469273440182137 f13=0.5034915244954404 f14=0.6071984234000622 f15=0.6982365591397849 max_index = 5 \n",
      "\n",
      "\n",
      "107\n",
      "column=go_6784 best thre=0.98 f1=0.2031539888682746 f11=0.22105292776949267 f12=0.2867617107942973 f13=0.3975680676711605 f14=0.498160010903639 f15=0.6278075838865145 max_index = 5 \n",
      "\n",
      "\n",
      "108\n",
      "column=go_15974 best thre=0.98 f1=0.5900073475385746 f11=0.6198379004245465 f12=0.7150489759572574 f13=0.787819253438114 f14=0.8285123966942148 f15=0.8692810457516341 max_index = 5 \n",
      "\n",
      "\n",
      "109\n",
      "column=go_917 best thre=0.98 f1=0.30003910708157416 f11=0.35074360276007294 f12=0.5185367406592771 f13=0.6710121310881768 f14=0.74242853099349 f15=0.7787164906580016 max_index = 5 \n",
      "\n",
      "\n",
      "110\n",
      "column=go_42121 best thre=0.98 f1=0.5736558707707146 f11=0.6070820565202587 f12=0.6906125618871954 f13=0.7656385751520416 f14=0.817873435489531 f15=0.8490637174814629 max_index = 5 \n",
      "\n",
      "\n",
      "111\n",
      "column=go_1902604 best thre=0.98 f1=0.12470722289042223 f11=0.13706926557605292 f12=0.18134715025906736 f13=0.2702740590173127 f14=0.3625910310463779 f15=0.4993833082088529 max_index = 5 \n",
      "\n",
      "\n",
      "112\n",
      "column=go_15734 best thre=0.98 f1=0.46865940217934626 f11=0.5107913669064749 f12=0.6298596112311016 f13=0.7396166134185304 f14=0.8019559902200489 f15=0.8545971563981043 max_index = 5 \n",
      "\n",
      "\n",
      "113\n",
      "column=go_918 best thre=0.98 f1=0.6913274336283186 f11=0.7284088789404962 f12=0.8114796714152023 f13=0.887926120168738 f14=0.9287762906309752 f15=0.956886967263743 max_index = 5 \n",
      "\n",
      "\n",
      "114\n",
      "column=go_45150 best thre=0.98 f1=0.29926317778197625 f11=0.34169007996542033 f12=0.4942798934336311 f13=0.674929332463579 f14=0.7886451612903226 f15=0.866549088771311 max_index = 5 \n",
      "\n",
      "\n",
      "115\n",
      "column=go_31460 best thre=0.98 f1=0.3858498376106477 f11=0.41588239335027816 f12=0.49909150974562283 f13=0.5907710394827079 f14=0.6579308076100664 f15=0.7451594711448085 max_index = 5 \n",
      "\n",
      "\n",
      "116\n",
      "column=go_6367 best thre=0.98 f1=0.1394345789668767 f11=0.1607664446781166 f12=0.24064519954930916 f13=0.37713408785727986 f14=0.49050176863618505 f15=0.6177370030581041 max_index = 5 \n",
      "\n",
      "\n",
      "117\n",
      "column=go_6228 best thre=0.98 f1=0.2783659182105353 f11=0.3297667342799188 f12=0.5267328350283871 f13=0.749600709849157 f14=0.8553562839277126 f15=0.9124257686236188 max_index = 5 \n",
      "\n",
      "\n",
      "118\n",
      "column=go_9052 best thre=0.98 f1=0.2190605562307857 f11=0.25073891625615763 f12=0.3642346897824865 f13=0.5032440056417489 f14=0.6049595910754991 f15=0.7162814325628653 max_index = 5 \n",
      "\n",
      "\n",
      "119\n",
      "column=go_19379 best thre=0.98 f1=0.2825410631251463 f11=0.31734317343173435 f12=0.4275106263122855 f13=0.5593542260208927 f14=0.6604953308972796 f15=0.761428158983736 max_index = 5 \n",
      "\n",
      "\n",
      "120\n",
      "column=go_16260 best thre=0.98 f1=0.1776781227046043 f11=0.19189640768588134 f12=0.24225471050825986 f13=0.30690743102638113 f14=0.3632458233890215 f15=0.4433796205750049 max_index = 5 \n",
      "\n",
      "\n",
      "121\n",
      "column=go_15671 best thre=0.98 f1=0.1764189310165489 f11=0.2099055639589551 f12=0.33912699640689736 f13=0.5018470529856095 f14=0.5933045124870485 f15=0.6615532118887824 max_index = 5 \n",
      "\n",
      "\n",
      "122\n",
      "column=go_19284 best thre=0.98 f1=0.3340897878638382 f11=0.376627712854758 f12=0.5020062416406598 f13=0.6501930501930502 f14=0.7435954555580307 f15=0.834135224107369 max_index = 5 \n",
      "\n",
      "\n",
      "123\n",
      "column=go_6146 best thre=0.98 f1=0.2637953248665562 f11=0.2961645082543754 f12=0.39578689255461436 f13=0.5117398279130815 f14=0.5995336384834615 f15=0.7069825436408977 max_index = 5 \n",
      "\n",
      "\n",
      "124\n",
      "column=go_6388 best thre=0.98 f1=0.08546812176909822 f11=0.09797453321897472 f12=0.1458939135666299 f13=0.2289523496008559 f14=0.308456561922366 f15=0.4293526195096137 max_index = 5 \n",
      "\n",
      "\n",
      "125\n",
      "column=go_46168 best thre=0.98 f1=0.21365225178380562 f11=0.24259965793974475 f12=0.346655868902439 f13=0.4845045534864754 f14=0.5895512901995679 f15=0.7117567931127253 max_index = 5 \n",
      "\n",
      "\n",
      "126\n",
      "column=go_19333 best thre=0.98 f1=0.6073752711496746 f11=0.647590995160951 f12=0.7701552328492738 f13=0.8635978537136402 f14=0.9083333333333332 f15=0.9435133457479825 max_index = 5 \n",
      "\n",
      "\n",
      "127\n",
      "column=go_43953 best thre=0.98 f1=0.4916341327384272 f11=0.5260957216456358 f12=0.624477824193405 f13=0.7200658910738186 f14=0.7776164414162852 f15=0.8356830335439961 max_index = 5 \n",
      "\n",
      "\n",
      "128\n",
      "column=go_6788 best thre=0.98 f1=0.16558347476921803 f11=0.19062634370073098 f12=0.27499473055575074 f13=0.3852575850132356 f14=0.46795601061812664 f15=0.5594155844155844 max_index = 5 \n",
      "\n",
      "\n",
      "129\n",
      "column=go_42351 best thre=0.98 f1=0.10572248579628651 f11=0.11535020449897751 f12=0.14841338436844057 f13=0.19662243667068757 f14=0.24451152579582877 f15=0.3238255033557047 max_index = 5 \n",
      "\n",
      "\n",
      "130\n",
      "column=go_9294 best thre=0.98 f1=0.173544346212336 f11=0.2029214850882532 f12=0.3136750685330293 f13=0.4670934110161371 f14=0.5743955051670512 f15=0.6728235598036683 max_index = 5 \n",
      "\n",
      "\n",
      "131\n",
      "column=go_44205 best thre=0.98 f1=0.24745926762381032 f11=0.2796300803115113 f12=0.3850443599493029 f13=0.5119606272175804 f14=0.5992499147630412 f15=0.7061634805537244 max_index = 5 \n",
      "\n",
      "\n",
      "132\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "column=go_9229 best thre=0.98 f1=0.11729549441863157 f11=0.1379184529781994 f12=0.216548394176604 f13=0.33454502814258913 f14=0.43869014421161656 f15=0.5645178942760205 max_index = 5 \n",
      "\n",
      "\n",
      "133\n",
      "column=go_32968 best thre=0.98 f1=0.31489833455113514 f11=0.3589802678957223 f12=0.512579140286571 f13=0.6707712974364716 f14=0.7614876789332637 f15=0.8352507707346418 max_index = 5 \n",
      "\n",
      "\n",
      "134\n",
      "column=go_6229 best thre=0.98 f1=0.352334774821998 f11=0.3946698020772095 f12=0.5310569321729888 f13=0.6744528073362748 f14=0.7585791691751956 f15=0.8329863520703216 max_index = 5 \n",
      "\n",
      "\n",
      "135\n",
      "column=go_1680 best thre=0.98 f1=0.2468128505864355 f11=0.2747366661093463 f12=0.3651574803149606 f13=0.48330224441903846 f14=0.5752762936397571 f15=0.6821470036616952 max_index = 5 \n",
      "\n",
      "\n",
      "136\n",
      "column=go_19475 best thre=0.98 f1=0.08573403372961017 f11=0.09938749194068343 f12=0.15390138208672016 f13=0.24289230246965962 f14=0.3342546526626129 f15=0.4776937618147448 max_index = 5 \n",
      "\n",
      "\n",
      "137\n",
      "column=go_18160 best thre=0.98 f1=0.44259198423127455 f11=0.49450397829186404 f12=0.6437281178317035 f13=0.7680841736080667 f14=0.8248822167212329 f15=0.8689535388623534 max_index = 5 \n",
      "\n",
      "\n",
      "138\n",
      "column=go_35429 best thre=0.98 f1=0.3702888340965129 f11=0.41376250801005565 f12=0.5783451010399638 f13=0.7635970479010953 f14=0.851534005578202 f15=0.9056825527413763 max_index = 5 \n",
      "\n",
      "\n",
      "139\n",
      "column=go_1900753 best thre=0.98 f1=0.34904419945382825 f11=0.3843706391292213 f12=0.4832650755543002 f13=0.5895340751043116 f14=0.6590330788804071 f15=0.7403524425607853 max_index = 5 \n",
      "\n",
      "\n",
      "140\n",
      "column=go_18189 best thre=0.98 f1=0.1471114242358552 f11=0.17098093718906454 f12=0.26597685636449975 f13=0.41112493478348117 f14=0.5286552225152021 f15=0.6593934104298493 max_index = 5 \n",
      "\n",
      "\n",
      "141\n",
      "column=go_6102 best thre=0.98 f1=0.142967542503864 f11=0.16522774635308127 f12=0.24142451387371636 f13=0.3484608060933037 f14=0.4360360360360361 f15=0.5560144553433144 max_index = 5 \n",
      "\n",
      "\n",
      "142\n",
      "column=go_19281 best thre=0.98 f1=0.22240898498682052 f11=0.26061151079136685 f12=0.40330625623485816 f13=0.5764817349194652 f14=0.6841435479765844 f15=0.8055598455598456 max_index = 5 \n",
      "\n",
      "\n",
      "143\n",
      "column=go_19646 best thre=0.98 f1=0.5477938744459879 f11=0.5884738527214514 f12=0.6942642971447822 f13=0.7849111004324844 f14=0.8387832305539721 f15=0.8895505617977529 max_index = 5 \n",
      "\n",
      "\n",
      "144\n",
      "column=go_6809 best thre=0.98 f1=0.12277667984189723 f11=0.13582946160153048 f12=0.17841389864074172 f13=0.2383897174730205 f14=0.2887177968611193 f15=0.36173051267390893 max_index = 5 \n",
      "\n",
      "\n",
      "145\n",
      "column=go_19427 best thre=0.98 f1=0.675663909447105 f11=0.7070891269660361 f12=0.7844296208730895 f13=0.8556129985228951 f14=0.8882268140317156 f15=0.9132465467554407 max_index = 5 \n",
      "\n",
      "\n",
      "146\n",
      "column=go_71918 best thre=0.98 f1=0.05960890087660148 f11=0.0644666155026861 f12=0.08149107390620665 f13=0.10615375251080408 f14=0.13061968408262453 f15=0.17239599463198096 max_index = 5 \n",
      "\n",
      "\n",
      "147\n",
      "column=go_70329 best thre=0.98 f1=0.22041086812458585 f11=0.24262342973999412 f12=0.32076020556579077 f13=0.4143145161290322 f14=0.4871180347513482 f15=0.5929909141479697 max_index = 5 \n",
      "\n",
      "\n",
      "148\n",
      "column=go_6200 best thre=0.98 f1=0.32598830613491575 f11=0.36386712552781236 f12=0.4855210453739576 f13=0.6036575228595178 f14=0.6595715145065013 f15=0.7034263438654083 max_index = 5 \n",
      "\n",
      "\n",
      "149\n",
      "column=go_9450 best thre=0.98 f1=0.42037047037857106 f11=0.4399344225224716 f12=0.5012253321294983 f13=0.5754786997179754 f14=0.6411126748903054 f15=0.7259524707657488 max_index = 5 \n",
      "\n",
      "\n",
      "150\n",
      "column=go_43419 best thre=0.98 f1=0.28733476241514827 f11=0.3258523734738335 f12=0.45628298388247035 f13=0.6164596273291926 f14=0.7261205207973268 f15=0.836231298018601 max_index = 5 \n",
      "\n",
      "\n",
      "151\n",
      "column=go_9244 best thre=0.98 f1=0.23947635290863836 f11=0.2842962080603534 f12=0.4248063064587068 f13=0.56900761166164 f14=0.6346750973861006 f15=0.6681211159592755 max_index = 5 \n",
      "\n",
      "\n",
      "152\n",
      "column=go_6097 best thre=0.98 f1=0.147332441191171 f11=0.17610087218859383 f12=0.286484059272564 f13=0.45113325416903305 f14=0.5842865949722583 f15=0.727193059328718 max_index = 5 \n",
      "\n",
      "\n",
      "153\n",
      "column=go_46306 best thre=0.98 f1=0.31952107054818635 f11=0.35726473290458066 f12=0.4893770255671588 f13=0.6466201587683426 f14=0.7574637475120842 f15=0.8571428571428572 max_index = 5 \n",
      "\n",
      "\n",
      "154\n",
      "column=go_70409 best thre=0.98 f1=0.3400087706475661 f11=0.36775878442545107 f12=0.4510509926041262 f13=0.537104941505151 f14=0.5988538681948424 f15=0.6626708573934654 max_index = 5 \n",
      "\n",
      "\n",
      "155\n",
      "column=go_34258 best thre=0.98 f1=0.35082458770614694 f11=0.3738317757009346 f12=0.4482578093188296 f13=0.5478354203935599 f14=0.6351916959651767 f15=0.7494719903449664 max_index = 5 \n",
      "\n",
      "\n",
      "156\n",
      "column=go_71951 best thre=0.98 f1=0.36801628590816554 f11=0.4075993091537133 f12=0.5287479406919275 f13=0.6498357963875205 f14=0.7348475932682123 f15=0.8148593572526224 max_index = 5 \n",
      "\n",
      "\n",
      "157\n",
      "column=go_19645 best thre=0.98 f1=0.133953000262359 f11=0.148052703769899 f12=0.19936016164337428 f13=0.27658062988396875 f14=0.3539204603843387 f15=0.48673484511634796 max_index = 5 \n",
      "\n",
      "\n",
      "158\n",
      "column=go_30254 best thre=0.98 f1=0.32575036761525816 f11=0.3783838281998286 f12=0.5486503452605148 f13=0.7123046875 f14=0.7891592920353983 f15=0.8568606876118757 max_index = 5 \n",
      "\n",
      "\n",
      "159\n",
      "column=go_46294 best thre=0.98 f1=0.39698302954116904 f11=0.44823178525777585 f12=0.6055063534847902 f13=0.7513559117753404 f14=0.8291896250503964 f15=0.8993209329790375 max_index = 5 \n",
      "\n",
      "\n",
      "160\n",
      "column=go_30420 best thre=0.98 f1=0.283161313008046 f11=0.3179301037740239 f12=0.43038455451811986 f13=0.5665529010238908 f14=0.662038655546122 f15=0.780996242879651 max_index = 5 \n",
      "\n",
      "\n",
      "161\n",
      "column=go_10501 best thre=0.98 f1=0.8479827089337175 f11=0.8679941002949852 f12=0.9165366614664585 f13=0.9459241323648104 f14=0.9628712871287128 f15=0.9722689075630251 max_index = 5 \n",
      "\n",
      "\n",
      "162\n",
      "column=go_6556 best thre=0.98 f1=0.324086482623068 f11=0.38269383434771104 f12=0.5752019386106624 f13=0.7616699539776464 f14=0.8485486166986446 f15=0.9045693489688637 max_index = 5 \n",
      "\n",
      "\n",
      "163\n",
      "column=go_19285 best thre=0.98 f1=0.3697636321085888 f11=0.4000810290691786 f12=0.4987360970677451 f13=0.6114689221696283 f14=0.6912101462039809 f15=0.7765480107687706 max_index = 5 \n",
      "\n",
      "\n",
      "164\n",
      "column=go_43215 best thre=0.98 f1=0.34881487845555165 f11=0.38441355440183106 f12=0.4828900021167007 f13=0.5896365849417493 f14=0.6595494613124387 f15=0.7407076682665477 max_index = 5 \n",
      "\n",
      "\n",
      "165\n",
      "column=go_19358 best thre=0.98 f1=0.24191968658178256 f11=0.2662955423538698 f12=0.34889353737751844 f13=0.46257083209066513 f14=0.5548741418764301 f15=0.6618721726017864 max_index = 5 \n",
      "\n",
      "\n",
      "166\n",
      "column=go_8612 best thre=0.98 f1=0.06256577753583742 f11=0.0708489516826626 f12=0.10175774243692454 f13=0.1490870413047332 f14=0.1897893772893773 f15=0.26164519326065416 max_index = 5 \n",
      "\n",
      "\n",
      "167\n",
      "column=go_10133 best thre=0.98 f1=0.451449608835711 f11=0.49184932537493103 f12=0.6017720895889737 f13=0.7028980270289803 f14=0.7765897580191334 f15=0.8435519857206605 max_index = 5 \n",
      "\n",
      "\n",
      "168\n",
      "column=go_19746 best thre=0.98 f1=0.3380474452554745 f11=0.373077892614066 f12=0.48702594810379235 f13=0.606239460370995 f14=0.6958661417322834 f15=0.7541738629821532 max_index = 5 \n",
      "\n",
      "\n",
      "169\n",
      "column=go_19551 best thre=0.98 f1=0.12414343026587923 f11=0.14701523504653508 f12=0.2429169992019154 f13=0.4005069708491762 f14=0.5289970434387082 f15=0.6611329547109364 max_index = 5 \n",
      "\n",
      "\n",
      "170\n",
      "column=go_9758 best thre=0.98 f1=0.16969476744186046 f11=0.18778303310858407 f12=0.24729603418346907 f13=0.3206405693950178 f14=0.37669902912621356 f15=0.45099089448312796 max_index = 5 \n",
      "\n",
      "\n",
      "171\n",
      "column=go_48473 best thre=0.98 f1=0.6404350607805502 f11=0.6825775656324582 f12=0.7766990291262135 f13=0.870362287210825 f14=0.9049567985447932 f15=0.9327058823529412 max_index = 5 \n",
      "\n",
      "\n",
      "172\n",
      "column=go_6168 best thre=0.98 f1=0.584531913804576 f11=0.6307760336060138 f12=0.7641927258086162 f13=0.8690384910289832 f14=0.9126055880441845 f15=0.9383011451034011 max_index = 5 \n",
      "\n",
      "\n",
      "173\n",
      "column=go_6878 best thre=0.98 f1=0.22403977227167027 f11=0.24640620866037566 f12=0.32156048014773775 f13=0.4132280649486072 f14=0.48755516328331866 f15=0.5874785591766724 max_index = 5 \n",
      "\n",
      "\n",
      "174\n",
      "column=go_31388 best thre=0.98 f1=0.30175606896367946 f11=0.3493754477774663 f12=0.5116646528487777 f13=0.6826892264308146 f14=0.7710629484429321 f15=0.8398902104300092 max_index = 5 \n",
      "\n",
      "\n",
      "175\n",
      "column=go_42906 best thre=0.98 f1=0.1756338899196042 f11=0.19712440091685768 f12=0.27553998832457677 f13=0.4082104654524429 f14=0.5066618653222902 f15=0.6290974405029187 max_index = 5 \n",
      "\n",
      "\n",
      "176\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "column=go_33539 best thre=0.98 f1=0.3443203108736382 f11=0.3876417676965193 f12=0.5280584820468716 f13=0.6762409369771334 f14=0.7682692307692307 f15=0.851608187134503 max_index = 5 \n",
      "\n",
      "\n",
      "177\n",
      "column=go_15757 best thre=0.98 f1=0.20871784802670332 f11=0.23016130778391256 f12=0.2920573634859349 f13=0.36464279536412386 f14=0.42653061224489797 f15=0.49266144814090024 max_index = 5 \n",
      "\n",
      "\n",
      "178\n",
      "column=go_19516 best thre=0.98 f1=0.4144075102979212 f11=0.44674250258531545 f12=0.5343908786714587 f13=0.6239092495636999 f14=0.6809933142311366 f15=0.7507082152974505 max_index = 5 \n",
      "\n",
      "\n",
      "179\n",
      "column=go_42882 best thre=0.98 f1=0.17401677539608576 f11=0.19969133156134783 f12=0.2882635552505147 f13=0.4105745886161316 f14=0.5058035714285715 f15=0.6245467224546722 max_index = 5 \n",
      "\n",
      "\n",
      "180\n",
      "column=go_45151 best thre=0.98 f1=0.20610817266678041 f11=0.23610748760761804 f12=0.34198763670946264 f13=0.4848733744010951 f14=0.5930941096817874 f15=0.7086712414223331 max_index = 5 \n",
      "\n",
      "\n",
      "181\n",
      "column=go_6122 best thre=0.98 f1=0.5227745664739885 f11=0.5553369404820463 f12=0.6500360490266762 f13=0.751170568561873 f14=0.8143429195485985 f15=0.8741878322504429 max_index = 5 \n",
      "\n",
      "\n",
      "182\n",
      "column=go_9239 best thre=0.98 f1=0.10636129073360304 f11=0.1259836532978521 f12=0.20445013606531134 f13=0.3222976948173503 f14=0.4193258426966292 f15=0.5248150927216207 max_index = 5 \n",
      "\n",
      "\n",
      "183\n",
      "column=go_44209 best thre=0.98 f1=0.32310364794441226 f11=0.3522595596755504 f12=0.4540997003541269 f13=0.600216684723727 f14=0.6903765690376568 f15=0.7904599659284497 max_index = 5 \n",
      "\n",
      "\n",
      "184\n",
      "column=go_9271 best thre=0.98 f1=0.39345920431557657 f11=0.41524216524216523 f12=0.48410554747558693 f13=0.5627873215581901 f14=0.6229860365198711 f15=0.7076828895010714 max_index = 5 \n",
      "\n",
      "\n",
      "185\n",
      "column=go_51085 best thre=0.98 f1=0.7635369188696445 f11=0.7906184982031398 f12=0.8504482477587613 f13=0.8904109589041096 f14=0.9128612397970439 f15=0.9252925292529254 max_index = 5 \n",
      "\n",
      "\n",
      "186\n",
      "column=go_46656 best thre=0.98 f1=0.21123491179201484 f11=0.24203708632774376 f12=0.3539234231296067 f13=0.48777818314483773 f14=0.5947344734473446 f15=0.7135775268521068 max_index = 5 \n",
      "\n",
      "\n",
      "187\n",
      "column=go_6203 best thre=0.98 f1=0.2988838362959901 f11=0.3204077110569466 f12=0.39812775330396477 f13=0.4832887700534759 f14=0.5429966203529853 f15=0.6412416851441242 max_index = 5 \n",
      "\n",
      "\n",
      "188\n",
      "column=go_15756 best thre=0.98 f1=0.33167147730254526 f11=0.3572035097650722 f12=0.43938332165381916 f13=0.5409159973952681 f14=0.6060014637716516 f15=0.7000285959393766 max_index = 5 \n",
      "\n",
      "\n",
      "189\n",
      "column=go_34194 best thre=0.98 f1=0.11133773245350931 f11=0.12655459887932213 f12=0.18639008348090058 f13=0.2751274442669101 f14=0.36138463100192875 f15=0.49775979187743896 max_index = 5 \n",
      "\n",
      "\n",
      "190\n",
      "column=go_6212 best thre=0.98 f1=0.20073597056117756 f11=0.2223128696716296 f12=0.30325214698014924 f13=0.4203846916517946 f14=0.5192878338278931 f15=0.6535906358747231 max_index = 5 \n",
      "\n",
      "\n",
      "191\n",
      "column=go_15707 best thre=0.98 f1=0.2721631205673759 f11=0.2921027592768791 f12=0.36286587168621265 f13=0.43796141938556793 f14=0.4989010989010989 f15=0.5812408877369188 max_index = 5 \n",
      "\n",
      "\n",
      "192\n",
      "column=go_15796 best thre=0.98 f1=0.17739318550567876 f11=0.19017534811758638 f12=0.23682938617689706 f13=0.31643924626380765 f14=0.38954423592493304 f15=0.5307116104868914 max_index = 5 \n",
      "\n",
      "\n",
      "193\n",
      "column=go_9246 best thre=0.98 f1=0.14829626827356984 f11=0.17118776338379896 f12=0.2569145666871543 f13=0.3849589396340585 f14=0.4877772534214252 f15=0.6080320280245215 max_index = 5 \n",
      "\n",
      "\n",
      "194\n",
      "column=go_19569 best thre=0.98 f1=0.14232247622901073 f11=0.16123853211009173 f12=0.22759069464779566 f13=0.3256250733654185 f14=0.4067495559502664 f15=0.5329141285124779 max_index = 5 \n",
      "\n",
      "\n",
      "195\n",
      "column=go_18339 best thre=0.98 f1=0.47828263862811365 f11=0.5138701858335577 f12=0.6144500242993682 f13=0.7112836046951911 f14=0.7730761252852105 f15=0.8407322654462243 max_index = 5 \n",
      "\n",
      "\n",
      "196\n",
      "column=go_51131 best thre=0.98 f1=0.434850320124589 f11=0.48717450446949084 f12=0.6402368387179816 f13=0.7768490774325816 f14=0.8300035050823695 f15=0.8750719631548647 max_index = 5 \n",
      "\n",
      "\n",
      "197\n",
      "column=go_32218 best thre=0.98 f1=0.28612328889684174 f11=0.30524004963252843 f12=0.35985146843704285 f13=0.4210943695479778 f14=0.473919523099851 f15=0.5373337879304467 max_index = 5 \n",
      "\n",
      "\n",
      "198\n",
      "column=go_71766 best thre=0.98 f1=0.11569526304300372 f11=0.12511818468326505 f12=0.16142857142857145 f13=0.2152834839769926 f14=0.2630247850278199 f15=0.3363576012392122 max_index = 5 \n",
      "\n",
      "\n",
      "199\n",
      "column=go_51365 best thre=0.98 f1=0.18073207930859175 f11=0.19020866773675763 f12=0.21571319603356215 f13=0.24730993120479802 f14=0.2737254901960784 f15=0.31137184115523464 max_index = 5 \n",
      "\n",
      "\n",
      "200\n",
      "column=go_1901023 best thre=0.98 f1=0.12512340487769208 f11=0.13972121162572046 f12=0.19383460357183482 f13=0.27870332375871976 f14=0.3598638008086827 f15=0.49323131253678637 max_index = 5 \n",
      "\n",
      "\n",
      "201\n",
      "column=go_42919 best thre=0.98 f1=0.189691934416587 f11=0.21000285329963722 f12=0.2823933137764986 f13=0.39043329709582236 f14=0.4927738463071863 f15=0.6370192307692308 max_index = 5 \n",
      "\n",
      "\n",
      "202\n",
      "column=go_19251 best thre=0.98 f1=0.06903902918989832 f11=0.08001527737992935 f12=0.12142804126772064 f13=0.18434373160682754 f14=0.24826248585744304 f15=0.3605661782964986 max_index = 5 \n",
      "\n",
      "\n",
      "203\n",
      "column=go_9447 best thre=0.98 f1=0.25492324391378507 f11=0.2729536775693176 f12=0.3381324557795146 f13=0.4192229038854806 f14=0.49396863691194204 f15=0.6198786039453719 max_index = 5 \n",
      "\n",
      "\n",
      "204\n",
      "column=go_35998 best thre=0.98 f1=0.2296312983404671 f11=0.24975482183720174 f12=0.32004197271773344 f13=0.41329519139081866 f14=0.48763936015511394 f15=0.576597836774828 max_index = 5 \n",
      "\n",
      "\n",
      "205\n",
      "column=go_19512 best thre=0.98 f1=0.16758019160192553 f11=0.19289284341187454 f12=0.2839996756143054 f13=0.41617664706588264 f14=0.5258172930033609 f15=0.6690590111642744 max_index = 5 \n",
      "\n",
      "\n",
      "206\n",
      "column=go_19629 best thre=0.98 f1=0.3494044242768009 f11=0.3905957987590641 f12=0.5229876279211854 f13=0.6679544394857789 f14=0.7574203574203574 f15=0.8216745197535338 max_index = 5 \n",
      "\n",
      "\n",
      "207\n",
      "column=go_43571 best thre=0.98 f1=0.13542139506733242 f11=0.15068262261924828 f12=0.2081134304844427 f13=0.2909668633681343 f14=0.35943612074691855 f15=0.4486761710794298 max_index = 5 \n",
      "\n",
      "\n",
      "208\n",
      "column=go_9772 best thre=0.98 f1=0.1577002660360627 f11=0.1698188492538505 f12=0.21777914487849892 f13=0.28521458361361496 f14=0.3443104010433649 f15=0.4420519316022799 max_index = 5 \n",
      "\n",
      "\n",
      "209\n",
      "column=go_42956 best thre=0.98 f1=0.24166239096972808 f11=0.2625488008923591 f12=0.3362895279046851 f13=0.43001608825557347 f14=0.5117817279867715 f15=0.61715445779056 max_index = 5 \n",
      "\n",
      "\n",
      "210\n",
      "column=go_15889 best thre=0.98 f1=0.3435505925250683 f11=0.3784225069078121 f12=0.4929751047572098 f13=0.6358295053947228 f14=0.730630965662576 f15=0.8597218111867417 max_index = 5 \n",
      "\n",
      "\n",
      "211\n",
      "column=go_19295 best thre=0.98 f1=0.045644757407189554 f11=0.051632149618495785 f12=0.07512325800108297 f13=0.1146461623463327 f14=0.16003277121594864 f15=0.24363131079203335 max_index = 5 \n",
      "\n",
      "\n",
      "212\n",
      "column=go_19545 best thre=0.98 f1=0.2855537870985863 f11=0.3122630877983813 f12=0.3911305917236044 f13=0.48899676375404527 f14=0.5576994434137291 f15=0.6565498772595404 max_index = 5 \n",
      "\n",
      "\n",
      "213\n",
      "column=go_46392 best thre=0.98 f1=0.5626262626262627 f11=0.6055993476488176 f12=0.752710027100271 f13=0.87199684915321 f14=0.919482686691698 f15=0.9518438177874187 max_index = 5 \n",
      "\n",
      "\n",
      "214\n",
      "column=go_15871 best thre=0.98 f1=0.39204503911676536 f11=0.43523484479115876 f12=0.5688073394495413 f13=0.7056623931623931 f14=0.7826217678893567 f15=0.8466644540325257 max_index = 5 \n",
      "\n",
      "\n",
      "215\n",
      "column=go_19491 best thre=0.98 f1=0.13793718772305497 f11=0.1596725727903844 f12=0.2442453701843892 f13=0.3767580983898683 f14=0.4867429055484964 f15=0.6181359623982575 max_index = 5 \n",
      "\n",
      "\n",
      "216\n",
      "column=go_31412 best thre=0.98 f1=0.06310707185458676 f11=0.07172954751717878 f12=0.10565982689876495 f13=0.16397183209410282 f14=0.22373147732375398 f15=0.3234978540772533 max_index = 5 \n",
      "\n",
      "\n",
      "217\n",
      "column=go_6069 best thre=0.98 f1=0.9102236421725239 f11=0.9262028608582574 f12=0.9490169943352216 f13=0.9693669162695713 f14=0.9785260264559353 f15=0.9844236760124612 max_index = 5 \n",
      "\n",
      "\n",
      "218\n",
      "column=go_15990 best thre=0.98 f1=0.37617457482332844 f11=0.406242134407249 f12=0.5062787777312683 f13=0.6346659652814308 f14=0.7221722172217222 f15=0.8378095570282525 max_index = 5 \n",
      "\n",
      "\n",
      "219\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "column=go_45982 best thre=0.98 f1=0.6175410492591109 f11=0.6508975712777191 f12=0.7415080703444954 f13=0.8256782164920763 f14=0.8747503566333809 f15=0.914560770156438 max_index = 5 \n",
      "\n",
      "\n",
      "220\n",
      "column=go_1901106 best thre=0.98 f1=0.15953165020124405 f11=0.1772234450150811 f12=0.2388460248238846 f13=0.33360141547370115 f14=0.425280067638977 f15=0.5583358456436538 max_index = 5 \n",
      "\n",
      "\n",
      "221\n",
      "column=go_6742 best thre=0.98 f1=0.10943184998600618 f11=0.12005856515373352 f12=0.16087182148417228 f13=0.21191310441485636 f14=0.2534538632099608 f15=0.31793057705063005 max_index = 5 \n",
      "\n",
      "\n",
      "222\n",
      "column=go_42122 best thre=0.98 f1=0.10322782826700093 f11=0.11198910081743868 f12=0.14037657805161788 f13=0.1770234409553295 f14=0.21073771589235507 f15=0.26307090039147457 max_index = 5 \n",
      "\n",
      "\n",
      "223\n",
      "column=go_9298 best thre=0.98 f1=0.1203996242848604 f11=0.14012230789683597 f12=0.2143152241272464 f13=0.3294839699588145 f14=0.4300703082747431 f15=0.5553776742832193 max_index = 5 \n",
      "\n",
      "\n",
      "224\n",
      "column=go_30253 best thre=0.98 f1=0.2288520474098143 f11=0.2552858261550509 f12=0.35160572699788945 f13=0.4827695226051217 f14=0.585006303947241 f15=0.7054892601431981 max_index = 5 \n",
      "\n",
      "\n",
      "225\n",
      "column=go_6120 best thre=0.98 f1=0.2540809084457062 f11=0.2776397515527951 f12=0.3636735528737984 f13=0.4854155200880572 f14=0.5697560975609756 f15=0.6641250476553565 max_index = 5 \n",
      "\n",
      "\n",
      "226\n",
      "column=go_8218 best thre=0.98 f1=0.04136999409000085 f11=0.0464754774443246 f12=0.06519357195032871 f13=0.0950180303189441 f14=0.12532586656367672 f15=0.18222052651659673 max_index = 5 \n",
      "\n",
      "\n",
      "227\n",
      "column=go_34229 best thre=0.98 f1=0.08857376602127079 f11=0.09453781512605042 f12=0.11474591974912694 f13=0.14930393347659535 f14=0.18600303773805352 f15=0.2921390076194016 max_index = 5 \n",
      "\n",
      "\n",
      "228\n",
      "column=go_51391 best thre=0.98 f1=0.33347807207989577 f11=0.35739791957770534 f12=0.4182718767198679 f13=0.4820819112627986 f14=0.5337132003798671 f15=0.5943472409152086 max_index = 5 \n",
      "\n",
      "\n",
      "229\n",
      "column=go_7156 best thre=0.98 f1=0.13401640865723727 f11=0.14536219615431129 f12=0.18228902203716868 f13=0.2296434788214815 f14=0.2685143116765107 f15=0.32693568017975844 max_index = 5 \n",
      "\n",
      "\n",
      "230\n",
      "column=go_6021 best thre=0.98 f1=0.09861087866108785 f11=0.112452569851673 f12=0.16180684308107196 f13=0.2405277401894452 f14=0.31200089978630074 f15=0.41638011878712106 max_index = 5 \n",
      "\n",
      "\n",
      "231\n",
      "column=go_70395 best thre=0.98 f1=0.33040259139287365 f11=0.35469448584202684 f12=0.42441054091539526 f13=0.5090390104662226 f14=0.57357276869472 f15=0.6725329981143934 max_index = 5 \n",
      "\n",
      "\n",
      "232\n",
      "column=go_921 best thre=0.98 f1=0.4751612340944744 f11=0.5134676963646637 f12=0.6177272727272728 f13=0.7266342517558076 f14=0.7961802447030738 f15=0.8701938876109102 max_index = 5 \n",
      "\n",
      "\n",
      "233\n",
      "column=go_46411 best thre=0.98 f1=0.08745186136071886 f11=0.09198023565184342 f12=0.11057888300040768 f13=0.14018025027556247 f14=0.16923076923076924 f15=0.23622941510505394 max_index = 5 \n",
      "\n",
      "\n",
      "234\n",
      "column=go_71586 best thre=0.98 f1=0.21865994236311237 f11=0.230513623848856 f12=0.2686467915327496 f13=0.31746449237243557 f14=0.3571640462770691 f15=0.4074957410562181 max_index = 5 \n",
      "\n",
      "\n",
      "235\n",
      "column=go_10125 best thre=0.98 f1=0.30236039147956245 f11=0.32780708364732475 f12=0.4080917489646384 f13=0.5070943861813696 f14=0.5772537460083517 f15=0.6471466985360024 max_index = 5 \n",
      "\n",
      "\n",
      "236\n",
      "column=go_9436 best thre=0.98 f1=0.21408695652173912 f11=0.24040621033102239 f12=0.32694355697550587 f13=0.4432784512393704 f14=0.5362637362637362 f15=0.6679504814305365 max_index = 5 \n",
      "\n",
      "\n",
      "237\n",
      "column=go_50787 best thre=0.98 f1=0.08758770868618437 f11=0.09189566541854413 f12=0.10474941724941725 f13=0.12309808303962863 f14=0.13881296368606014 f15=0.1645436484749328 max_index = 5 \n",
      "\n",
      "\n",
      "238\n",
      "column=go_6015 best thre=0.98 f1=0.17829564258135686 f11=0.1997989638908219 f12=0.2738474612279584 f13=0.3745449250036406 f14=0.45486851457000715 f15=0.5587651598676957 max_index = 5 \n",
      "\n",
      "\n",
      "239\n",
      "column=go_51692 best thre=0.98 f1=0.5027548209366391 f11=0.5538227462913656 f12=0.7305569493226293 f13=0.8401615695326023 f14=0.8876678876678877 f15=0.9220945083014048 max_index = 5 \n",
      "\n",
      "\n",
      "240\n",
      "column=go_43093 best thre=0.98 f1=0.5017144424633109 f11=0.5464595159844636 f12=0.6761552680221811 f13=0.7900912646675359 f14=0.8486549707602339 f15=0.9104704097116845 max_index = 5 \n",
      "\n",
      "\n",
      "241\n",
      "column=go_30153 best thre=0.98 f1=0.08172996205634467 f11=0.09056716195583361 f12=0.12102738356985808 f13=0.1704714114071666 f14=0.21913552042401535 f15=0.30597112007285027 max_index = 5 \n",
      "\n",
      "\n",
      "242\n",
      "column=go_52837 best thre=0.98 f1=0.545774647887324 f11=0.5976434228317559 f12=0.7284768211920529 f13=0.8457079768147944 f14=0.9051878354203936 f15=0.9432892249527409 max_index = 5 \n",
      "\n",
      "\n",
      "243\n",
      "column=go_6171 best thre=0.98 f1=0.3420701910262106 f11=0.37998764669549107 f12=0.4986970684039088 f13=0.6293532338308457 f14=0.7212801528540722 f15=0.8230407089448907 max_index = 5 \n",
      "\n",
      "\n",
      "244\n",
      "column=go_2949 best thre=0.98 f1=0.05295658385586158 f11=0.06009236620483564 f12=0.08437240232751454 f13=0.11562661929084314 f14=0.13901847359582012 f15=0.16272801353977306 max_index = 5 \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_curve, f1_score, accuracy_score\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "# test_dataset_loader = DatasetLoader(X_test, y_test)\n",
    "# test_loader = DataLoader(test_dataset_loader, batch_size=800, shuffle=True, num_workers=0)\n",
    "\n",
    "# train_dataset_loader2 = DatasetLoader(X_train, y_train)\n",
    "# train_loader3 = DataLoader(train_dataset_loader2, batch_size=800, shuffle=True, num_workers=0)\n",
    "\n",
    "all_labels = []\n",
    "all_predictions = []\n",
    "\n",
    "label_to_threshold = {}\n",
    "\n",
    "with torch.no_grad():\n",
    "    for label_index, column in enumerate(Y.columns):\n",
    "        \n",
    "    #for column, _ in labels_on_sums1.items():\n",
    "        \n",
    "        label_index = Y.columns.tolist().index(column)\n",
    "        print(label_index)\n",
    "        threshold = 0.4 #go_term_threhold[column][0]\n",
    "        current_labels = []\n",
    "        curent_predictions =[]\n",
    "        \n",
    "        curent_predictions1 =[]\n",
    "        curent_predictions2 =[]\n",
    "        curent_predictions3 =[]\n",
    "        curent_predictions4 =[]\n",
    "        curent_predictions5 =[]\n",
    "        pred_probs = []\n",
    "#         for  index, (images, labels) in enumerate(train_loader3):\n",
    "#             images = images.float().to(device)\n",
    "#             labels = labels.float().to(device)\n",
    "#             images = Variable(images)\n",
    "#             labels = Variable(labels)\n",
    "#             outputs = model(images)\n",
    "#             probs = torch.sigmoid(outputs)\n",
    "#             probs = probs.cpu().numpy()\n",
    "            \n",
    "        for ind1, labels in enumerate(all_labels1):\n",
    "            probs = all_probs[ind1]\n",
    "                \n",
    "            pred_probs.extend(probs[:, label_index].tolist())\n",
    "\n",
    "#             all_labels.extend(labels[:, label_index].tolist())\n",
    "            current_labels.extend(labels[:, label_index].tolist())\n",
    "        \n",
    "\n",
    "        \n",
    "            predictions = [1 if m > threshold else 0 for m in probs[:, label_index].tolist()]\n",
    "            curent_predictions.extend(predictions)\n",
    "            \n",
    "            \n",
    "            predictions = [1 if m > 0.5 else 0 for m in probs[:, label_index].tolist()]\n",
    "            curent_predictions1.extend(predictions)\n",
    "            predictions = [1 if m > 0.75 else 0 for m in probs[:, label_index].tolist()]\n",
    "            curent_predictions2.extend(predictions)\n",
    "            predictions = [1 if m > 0.9 else 0 for m in probs[:, label_index].tolist()]\n",
    "            curent_predictions3.extend(predictions)\n",
    "\n",
    "\n",
    "            predictions = [1 if m > 0.95 else 0 for m in probs[:, label_index].tolist()]\n",
    "            curent_predictions4.extend(predictions)\n",
    "\n",
    "            predictions = [1 if m > 0.98 else 0 for m in probs[:, label_index].tolist()]\n",
    "            curent_predictions5.extend(predictions)\n",
    "#             all_predictions.extend(predictions)\n",
    "        \n",
    "        res = f1_score(current_labels, curent_predictions)\n",
    "        res1 = f1_score(current_labels, curent_predictions1)\n",
    "        res2 = f1_score(current_labels, curent_predictions2)\n",
    "        res3 = f1_score(current_labels, curent_predictions3)\n",
    "        res4 = f1_score(current_labels, curent_predictions4)\n",
    "        res5 = f1_score(current_labels, curent_predictions5)\n",
    "        \n",
    "        max_index = np.argmax(np.array([res, res1, res2, res3, res4, res5]))\n",
    "        \n",
    "        best_t = [threshold, 0.5, 0.75, 0.9,  0.95, 0.98 ][max_index]\n",
    "        \n",
    "        \n",
    "        \n",
    "        label_to_threshold[column] = best_t\n",
    "        \n",
    "        print('column={0} best thre={1} f1={2} f11={3} f12={4} f13={5} f14={6} f15={7} max_index = {8} '\n",
    "              .format(column, best_t, res,res1, res2, res3, res4, res5, max_index))\n",
    "        \n",
    "\n",
    "        print('\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('label_to_threshold.json', 'w') as outfile:\n",
    "    json.dump(label_to_threshold, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset_loader3 = DatasetLoader(X_test, y_test)\n",
    "test_loader3 = DataLoader(test_dataset_loader3, batch_size=800, shuffle=True, num_workers=0)\n",
    "test_all_probs = []\n",
    "test_all_labels = []\n",
    "with torch.no_grad():\n",
    "    for  index, (images, labels) in enumerate(test_loader3):\n",
    "        images = images.float().to(device)\n",
    "        labels = labels.float().to(device)\n",
    "        images = Variable(images)\n",
    "        labels = Variable(labels)\n",
    "        outputs = model(images)\n",
    "        probs = torch.sigmoid(outputs)\n",
    "        probs = probs.cpu().numpy()\n",
    "\n",
    "        test_all_probs.append(probs)\n",
    "        #labels[:, label_index].tolist()\n",
    "        test_all_labels.append(labels)\n",
    "\n",
    "#label_to_threshold[column]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, f1_score, accuracy_score,hamming_loss,jaccard_score,\\\n",
    "log_loss, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "multiclass-multioutput is not supported",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-8680d27b529a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#jaccard_score(current_labels, curent_predictions, average='samples')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprecision_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'samples'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m# len(current_labels),len( curent_predictions)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# current_labels[0:10],curent_predictions[0:10]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36mprecision_score\u001b[0;34m(y_true, y_pred, labels, pos_label, average, sample_weight)\u001b[0m\n\u001b[1;32m   1567\u001b[0m                                                  \u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maverage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1568\u001b[0m                                                  \u001b[0mwarn_for\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'precision'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1569\u001b[0;31m                                                  sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m   1570\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36mprecision_recall_fscore_support\u001b[0;34m(y_true, y_pred, beta, labels, pos_label, average, warn_for, sample_weight)\u001b[0m\n\u001b[1;32m   1413\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"beta should be >0 in the F-beta score\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1414\u001b[0m     labels = _check_set_wise_labels(y_true, y_pred, average, labels,\n\u001b[0;32m-> 1415\u001b[0;31m                                     pos_label)\n\u001b[0m\u001b[1;32m   1416\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m     \u001b[0;31m# Calculate tp_sum, pred_sum, true_sum ###\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36m_check_set_wise_labels\u001b[0;34m(y_true, y_pred, average, labels, pos_label)\u001b[0m\n\u001b[1;32m   1237\u001b[0m                          str(average_options))\n\u001b[1;32m   1238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1239\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1240\u001b[0m     \u001b[0mpresent_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munique_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0maverage\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'binary'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;31m# No metrics support \"multiclass-multioutput\" format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0my_type\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"binary\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"multiclass\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"multilabel-indicator\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{0} is not supported\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"binary\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"multiclass\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: multiclass-multioutput is not supported"
     ]
    }
   ],
   "source": [
    "#jaccard_score(current_labels, curent_predictions, average='samples')\n",
    "# precision_score([[1,1,1],[1,0,1]], [[1,0,1],[1,0,1]], average = 'samples')\n",
    "# len(current_labels),len( curent_predictions)\n",
    "# current_labels[0:10],curent_predictions[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "\n",
      "\n",
      "1\n",
      "\n",
      "\n",
      "2\n",
      "\n",
      "\n",
      "3\n",
      "\n",
      "\n",
      "4\n",
      "\n",
      "\n",
      "5\n",
      "\n",
      "\n",
      "6\n",
      "\n",
      "\n",
      "7\n",
      "\n",
      "\n",
      "8\n",
      "\n",
      "\n",
      "9\n",
      "\n",
      "\n",
      "10\n",
      "\n",
      "\n",
      "11\n",
      "\n",
      "\n",
      "12\n",
      "\n",
      "\n",
      "13\n",
      "\n",
      "\n",
      "14\n",
      "\n",
      "\n",
      "15\n",
      "\n",
      "\n",
      "16\n",
      "\n",
      "\n",
      "17\n",
      "\n",
      "\n",
      "18\n",
      "\n",
      "\n",
      "19\n",
      "\n",
      "\n",
      "20\n",
      "\n",
      "\n",
      "21\n",
      "\n",
      "\n",
      "22\n",
      "\n",
      "\n",
      "23\n",
      "\n",
      "\n",
      "24\n",
      "\n",
      "\n",
      "25\n",
      "\n",
      "\n",
      "26\n",
      "\n",
      "\n",
      "27\n",
      "\n",
      "\n",
      "28\n",
      "\n",
      "\n",
      "29\n",
      "\n",
      "\n",
      "30\n",
      "\n",
      "\n",
      "31\n",
      "\n",
      "\n",
      "32\n",
      "\n",
      "\n",
      "33\n",
      "\n",
      "\n",
      "34\n",
      "\n",
      "\n",
      "35\n",
      "\n",
      "\n",
      "36\n",
      "\n",
      "\n",
      "37\n",
      "\n",
      "\n",
      "38\n",
      "\n",
      "\n",
      "39\n",
      "\n",
      "\n",
      "40\n",
      "\n",
      "\n",
      "41\n",
      "\n",
      "\n",
      "42\n",
      "\n",
      "\n",
      "43\n",
      "\n",
      "\n",
      "44\n",
      "\n",
      "\n",
      "45\n",
      "\n",
      "\n",
      "46\n",
      "\n",
      "\n",
      "47\n",
      "\n",
      "\n",
      "48\n",
      "\n",
      "\n",
      "49\n",
      "\n",
      "\n",
      "50\n",
      "\n",
      "\n",
      "51\n",
      "\n",
      "\n",
      "52\n",
      "\n",
      "\n",
      "53\n",
      "\n",
      "\n",
      "54\n",
      "\n",
      "\n",
      "55\n",
      "\n",
      "\n",
      "56\n",
      "\n",
      "\n",
      "57\n",
      "\n",
      "\n",
      "58\n",
      "\n",
      "\n",
      "59\n",
      "\n",
      "\n",
      "60\n",
      "\n",
      "\n",
      "61\n",
      "\n",
      "\n",
      "62\n",
      "\n",
      "\n",
      "63\n",
      "\n",
      "\n",
      "64\n",
      "\n",
      "\n",
      "65\n",
      "\n",
      "\n",
      "66\n",
      "\n",
      "\n",
      "67\n",
      "\n",
      "\n",
      "68\n",
      "\n",
      "\n",
      "69\n",
      "\n",
      "\n",
      "70\n",
      "\n",
      "\n",
      "71\n",
      "\n",
      "\n",
      "72\n",
      "\n",
      "\n",
      "73\n",
      "\n",
      "\n",
      "74\n",
      "\n",
      "\n",
      "75\n",
      "\n",
      "\n",
      "76\n",
      "\n",
      "\n",
      "77\n",
      "\n",
      "\n",
      "78\n",
      "\n",
      "\n",
      "79\n",
      "\n",
      "\n",
      "80\n",
      "\n",
      "\n",
      "81\n",
      "\n",
      "\n",
      "82\n",
      "\n",
      "\n",
      "83\n",
      "\n",
      "\n",
      "84\n",
      "\n",
      "\n",
      "85\n",
      "\n",
      "\n",
      "86\n",
      "\n",
      "\n",
      "87\n",
      "\n",
      "\n",
      "88\n",
      "\n",
      "\n",
      "89\n",
      "\n",
      "\n",
      "90\n",
      "\n",
      "\n",
      "91\n",
      "\n",
      "\n",
      "92\n",
      "\n",
      "\n",
      "93\n",
      "\n",
      "\n",
      "94\n",
      "\n",
      "\n",
      "95\n",
      "\n",
      "\n",
      "96\n",
      "\n",
      "\n",
      "97\n",
      "\n",
      "\n",
      "98\n",
      "\n",
      "\n",
      "99\n",
      "\n",
      "\n",
      "100\n",
      "\n",
      "\n",
      "101\n",
      "\n",
      "\n",
      "102\n",
      "\n",
      "\n",
      "103\n",
      "\n",
      "\n",
      "104\n",
      "\n",
      "\n",
      "105\n",
      "\n",
      "\n",
      "106\n",
      "\n",
      "\n",
      "107\n",
      "\n",
      "\n",
      "108\n",
      "\n",
      "\n",
      "109\n",
      "\n",
      "\n",
      "110\n",
      "\n",
      "\n",
      "111\n",
      "\n",
      "\n",
      "112\n",
      "\n",
      "\n",
      "113\n",
      "\n",
      "\n",
      "114\n",
      "\n",
      "\n",
      "115\n",
      "\n",
      "\n",
      "116\n",
      "\n",
      "\n",
      "117\n",
      "\n",
      "\n",
      "118\n",
      "\n",
      "\n",
      "119\n",
      "\n",
      "\n",
      "120\n",
      "\n",
      "\n",
      "121\n",
      "\n",
      "\n",
      "122\n",
      "\n",
      "\n",
      "123\n",
      "\n",
      "\n",
      "124\n",
      "\n",
      "\n",
      "125\n",
      "\n",
      "\n",
      "126\n",
      "\n",
      "\n",
      "127\n",
      "\n",
      "\n",
      "128\n",
      "\n",
      "\n",
      "129\n",
      "\n",
      "\n",
      "130\n",
      "\n",
      "\n",
      "131\n",
      "\n",
      "\n",
      "132\n",
      "\n",
      "\n",
      "133\n",
      "\n",
      "\n",
      "134\n",
      "\n",
      "\n",
      "135\n",
      "\n",
      "\n",
      "136\n",
      "\n",
      "\n",
      "137\n",
      "\n",
      "\n",
      "138\n",
      "\n",
      "\n",
      "139\n",
      "\n",
      "\n",
      "140\n",
      "\n",
      "\n",
      "141\n",
      "\n",
      "\n",
      "142\n",
      "\n",
      "\n",
      "143\n",
      "\n",
      "\n",
      "144\n",
      "\n",
      "\n",
      "145\n",
      "\n",
      "\n",
      "146\n",
      "\n",
      "\n",
      "147\n",
      "\n",
      "\n",
      "148\n",
      "\n",
      "\n",
      "149\n",
      "\n",
      "\n",
      "150\n",
      "\n",
      "\n",
      "151\n",
      "\n",
      "\n",
      "152\n",
      "\n",
      "\n",
      "153\n",
      "\n",
      "\n",
      "154\n",
      "\n",
      "\n",
      "155\n",
      "\n",
      "\n",
      "156\n",
      "\n",
      "\n",
      "157\n",
      "\n",
      "\n",
      "158\n",
      "\n",
      "\n",
      "159\n",
      "\n",
      "\n",
      "160\n",
      "\n",
      "\n",
      "161\n",
      "\n",
      "\n",
      "162\n",
      "\n",
      "\n",
      "163\n",
      "\n",
      "\n",
      "164\n",
      "\n",
      "\n",
      "165\n",
      "\n",
      "\n",
      "166\n",
      "\n",
      "\n",
      "167\n",
      "\n",
      "\n",
      "168\n",
      "\n",
      "\n",
      "169\n",
      "\n",
      "\n",
      "170\n",
      "\n",
      "\n",
      "171\n",
      "\n",
      "\n",
      "172\n",
      "\n",
      "\n",
      "173\n",
      "\n",
      "\n",
      "174\n",
      "\n",
      "\n",
      "175\n",
      "\n",
      "\n",
      "176\n",
      "\n",
      "\n",
      "177\n",
      "\n",
      "\n",
      "178\n",
      "\n",
      "\n",
      "179\n",
      "\n",
      "\n",
      "180\n",
      "\n",
      "\n",
      "181\n",
      "\n",
      "\n",
      "182\n",
      "\n",
      "\n",
      "183\n",
      "\n",
      "\n",
      "184\n",
      "\n",
      "\n",
      "185\n",
      "\n",
      "\n",
      "186\n",
      "\n",
      "\n",
      "187\n",
      "\n",
      "\n",
      "188\n",
      "\n",
      "\n",
      "189\n",
      "\n",
      "\n",
      "190\n",
      "\n",
      "\n",
      "191\n",
      "\n",
      "\n",
      "192\n",
      "\n",
      "\n",
      "193\n",
      "\n",
      "\n",
      "194\n",
      "\n",
      "\n",
      "195\n",
      "\n",
      "\n",
      "196\n",
      "\n",
      "\n",
      "197\n",
      "\n",
      "\n",
      "198\n",
      "\n",
      "\n",
      "199\n",
      "\n",
      "\n",
      "200\n",
      "\n",
      "\n",
      "201\n",
      "\n",
      "\n",
      "202\n",
      "\n",
      "\n",
      "203\n",
      "\n",
      "\n",
      "204\n",
      "\n",
      "\n",
      "205\n",
      "\n",
      "\n",
      "206\n",
      "\n",
      "\n",
      "207\n",
      "\n",
      "\n",
      "208\n",
      "\n",
      "\n",
      "209\n",
      "\n",
      "\n",
      "210\n",
      "\n",
      "\n",
      "211\n",
      "\n",
      "\n",
      "212\n",
      "\n",
      "\n",
      "213\n",
      "\n",
      "\n",
      "214\n",
      "\n",
      "\n",
      "215\n",
      "\n",
      "\n",
      "216\n",
      "\n",
      "\n",
      "217\n",
      "\n",
      "\n",
      "218\n",
      "\n",
      "\n",
      "219\n",
      "\n",
      "\n",
      "220\n",
      "\n",
      "\n",
      "221\n",
      "\n",
      "\n",
      "222\n",
      "\n",
      "\n",
      "223\n",
      "\n",
      "\n",
      "224\n",
      "\n",
      "\n",
      "225\n",
      "\n",
      "\n",
      "226\n",
      "\n",
      "\n",
      "227\n",
      "\n",
      "\n",
      "228\n",
      "\n",
      "\n",
      "229\n",
      "\n",
      "\n",
      "230\n",
      "\n",
      "\n",
      "231\n",
      "\n",
      "\n",
      "232\n",
      "\n",
      "\n",
      "233\n",
      "\n",
      "\n",
      "234\n",
      "\n",
      "\n",
      "235\n",
      "\n",
      "\n",
      "236\n",
      "\n",
      "\n",
      "237\n",
      "\n",
      "\n",
      "238\n",
      "\n",
      "\n",
      "239\n",
      "\n",
      "\n",
      "240\n",
      "\n",
      "\n",
      "241\n",
      "\n",
      "\n",
      "242\n",
      "\n",
      "\n",
      "243\n",
      "\n",
      "\n",
      "244\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "# test_dataset_loader = DatasetLoader(X_test, y_test)\n",
    "# test_loader = DataLoader(test_dataset_loader, batch_size=800, shuffle=True, num_workers=0)\n",
    "\n",
    "# train_dataset_loader2 = DatasetLoader(X_train, y_train)\n",
    "# train_loader3 = DataLoader(train_dataset_loader2, batch_size=800, shuffle=True, num_workers=0)\n",
    "\n",
    "all_test_predictions = []\n",
    "all_test_labels = []\n",
    "\n",
    "column_res = {}    \n",
    "with torch.no_grad():\n",
    "    for label_index, column in enumerate(Y.columns):\n",
    "        \n",
    "    #for column, _ in labels_on_sums1.items():\n",
    "        \n",
    "        label_index = Y.columns.tolist().index(column)\n",
    "        print(label_index)\n",
    "        threshold = label_to_threshold[column]\n",
    "  \n",
    "\n",
    "        curent_predictions = []\n",
    "        current_labels = []\n",
    "        current_prob = []\n",
    "        for ind1, labels in enumerate(test_all_labels):\n",
    "            probs = test_all_probs[ind1]\n",
    "                \n",
    "            #pred_probs.extend(probs[:, label_index].tolist())\n",
    "            \n",
    "            \n",
    "#             all_labels.extend(labels[:, label_index].tolist())\n",
    "            current_labels.extend(labels[:, label_index].tolist())\n",
    "        \n",
    "            all_test_labels.extend(labels[:, label_index].tolist())\n",
    "        \n",
    "            predictions = [1 if m > threshold else 0 for m in probs[:, label_index].tolist()]\n",
    "            \n",
    "            current_prob.extend(probs[:, label_index].tolist())\n",
    "            curent_predictions.extend(predictions)\n",
    "            all_test_predictions.extend(predictions)\n",
    "           \n",
    "        \n",
    "\n",
    "        print(\"F1 score w: %f\" % f1_score(current_labels, curent_predictions, average = 'weighted'))\n",
    "        \n",
    "\n",
    "        print('\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_test_labels), len(all_test_predictions)\n",
    "import json\n",
    "with open('all_test_labels.json', 'w') as outfile:\n",
    "    json.dump(all_test_labels, outfile)\n",
    "    \n",
    "with open('all_test_predictions.json', 'w') as outfile:\n",
    "    json.dump(all_test_predictions, outfile)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-58d5f4e6ee77>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# print(\"Recall score w: %f\" % recall_score(all_test_labels, all_test_predictions, average = 'weighted'))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# #         print(\"F1 score: %f\" % f1_score(current_labels, curent_predictions, average = 'samples'))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"F1 score w: %f\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mf1_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_test_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_test_predictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'weighted'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36mf1_score\u001b[0;34m(y_true, y_pred, labels, pos_label, average, sample_weight)\u001b[0m\n\u001b[1;32m   1057\u001b[0m     return fbeta_score(y_true, y_pred, 1, labels=labels,\n\u001b[1;32m   1058\u001b[0m                        \u001b[0mpos_label\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpos_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maverage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1059\u001b[0;31m                        sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m   1060\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1061\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36mfbeta_score\u001b[0;34m(y_true, y_pred, beta, labels, pos_label, average, sample_weight)\u001b[0m\n\u001b[1;32m   1180\u001b[0m                                                  \u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maverage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1181\u001b[0m                                                  \u001b[0mwarn_for\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'f-score'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1182\u001b[0;31m                                                  sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m   1183\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36mprecision_recall_fscore_support\u001b[0;34m(y_true, y_pred, beta, labels, pos_label, average, warn_for, sample_weight)\u001b[0m\n\u001b[1;32m   1413\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"beta should be >0 in the F-beta score\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1414\u001b[0m     labels = _check_set_wise_labels(y_true, y_pred, average, labels,\n\u001b[0;32m-> 1415\u001b[0;31m                                     pos_label)\n\u001b[0m\u001b[1;32m   1416\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m     \u001b[0;31m# Calculate tp_sum, pred_sum, true_sum ###\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36m_check_set_wise_labels\u001b[0;34m(y_true, y_pred, average, labels, pos_label)\u001b[0m\n\u001b[1;32m   1237\u001b[0m                          str(average_options))\n\u001b[1;32m   1238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1239\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1240\u001b[0m     \u001b[0mpresent_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munique_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0maverage\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'binary'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     70\u001b[0m     \"\"\"\n\u001b[1;32m     71\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m     \u001b[0mtype_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m     \u001b[0mtype_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/sklearn/utils/multiclass.py\u001b[0m in \u001b[0;36mtype_of_target\u001b[0;34m(y)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    252\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0;31m# Known to fail in numpy 1.3 for array of arrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m     \"\"\"\n\u001b[0;32m--> 538\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    539\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "        \n",
    "# print(\"Accuracy score: %f\" % accuracy_score(all_test_labels, all_test_predictions))\n",
    "# print(\"Hamming loss: %f\" % hamming_loss(all_test_labels, all_test_predictions))\n",
    "#         #print(\"Jaccard score: %f\" % jaccard_score(current_labels, curent_predictions, average='samples'))\n",
    "#         #print(\"Jaccard score w: %f\" % jaccard_score(current_labels, curent_predictions, average='weighted'))\n",
    "# print(\"Log-loss: %f\" % log_loss(all_test_labels, all_test_predictions))\n",
    "# #         print(\"Precision score: %f\" % precision_score(current_labels, curent_predictions, average = 'samples'))\n",
    "# print(\"Precision score w: %f\" % precision_score(all_test_labels, all_test_predictions, average = 'weighted'))\n",
    "# #         print(\"Recall score: %f\" % recall_score(current_labels, curent_predictions, average = 'samples'))\n",
    "# print(\"Recall score w: %f\" % recall_score(all_test_labels, all_test_predictions, average = 'weighted'))\n",
    "# #         print(\"F1 score: %f\" % f1_score(current_labels, curent_predictions, average = 'samples'))\n",
    "print(\"F1 score w: %f\" % f1_score(all_test_labels, all_test_predictions, average = 'weighted'))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " ...]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105606\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n",
      "2400\n",
      "2500\n",
      "2600\n",
      "2700\n",
      "2800\n",
      "2900\n",
      "3000\n",
      "3100\n",
      "3200\n",
      "3300\n",
      "3400\n",
      "3500\n",
      "3600\n",
      "3700\n",
      "3800\n",
      "3900\n",
      "4000\n",
      "4100\n",
      "4200\n",
      "4300\n",
      "4400\n",
      "4500\n",
      "4600\n",
      "4700\n",
      "4800\n",
      "4900\n",
      "5000\n",
      "5100\n",
      "5200\n",
      "5300\n",
      "5400\n",
      "5500\n",
      "5600\n",
      "5700\n",
      "5800\n",
      "5900\n",
      "6000\n",
      "6100\n",
      "6200\n",
      "6300\n",
      "6400\n",
      "6500\n",
      "6600\n",
      "6700\n",
      "6800\n",
      "6900\n",
      "7000\n",
      "7100\n",
      "7200\n",
      "7300\n",
      "7400\n",
      "7500\n",
      "7600\n",
      "7700\n",
      "7800\n",
      "7900\n",
      "8000\n",
      "8100\n",
      "8200\n",
      "8300\n",
      "8400\n",
      "8500\n",
      "8600\n",
      "8700\n",
      "8800\n",
      "8900\n",
      "9000\n",
      "9100\n",
      "9200\n",
      "9300\n",
      "9400\n",
      "9500\n",
      "9600\n",
      "9700\n",
      "9800\n",
      "9900\n",
      "10000\n",
      "10100\n",
      "10200\n",
      "10300\n",
      "10400\n",
      "10500\n",
      "10600\n",
      "10700\n",
      "10800\n",
      "10900\n",
      "11000\n",
      "11100\n",
      "11200\n",
      "11300\n",
      "11400\n",
      "11500\n",
      "11600\n",
      "11700\n",
      "11800\n",
      "11900\n",
      "12000\n",
      "12100\n",
      "12200\n",
      "12300\n",
      "12400\n",
      "12500\n",
      "12600\n",
      "12700\n",
      "12800\n",
      "12900\n",
      "13000\n",
      "13100\n",
      "13200\n",
      "13300\n",
      "13400\n",
      "13500\n",
      "13600\n",
      "13700\n",
      "13800\n",
      "13900\n",
      "14000\n",
      "14100\n",
      "14200\n",
      "14300\n",
      "14400\n",
      "14500\n",
      "14600\n",
      "14700\n",
      "14800\n",
      "14900\n",
      "15000\n",
      "15100\n",
      "15200\n",
      "15300\n",
      "15400\n",
      "15500\n",
      "15600\n",
      "15700\n",
      "15800\n",
      "15900\n",
      "16000\n",
      "16100\n",
      "16200\n",
      "16300\n",
      "16400\n",
      "16500\n",
      "16600\n",
      "16700\n",
      "16800\n",
      "16900\n",
      "17000\n",
      "17100\n",
      "17200\n",
      "17300\n",
      "17400\n",
      "17500\n",
      "17600\n",
      "17700\n",
      "17800\n",
      "17900\n",
      "18000\n",
      "18100\n",
      "18200\n",
      "18300\n",
      "18400\n",
      "18500\n",
      "18600\n",
      "18700\n",
      "18800\n",
      "18900\n",
      "19000\n",
      "19100\n",
      "19200\n",
      "19300\n",
      "19400\n",
      "19500\n",
      "19600\n",
      "19700\n",
      "19800\n",
      "19900\n",
      "20000\n",
      "20100\n",
      "20200\n",
      "20300\n",
      "20400\n",
      "20500\n",
      "20600\n",
      "20700\n",
      "20800\n",
      "20900\n",
      "21000\n",
      "21100\n",
      "21200\n",
      "21300\n",
      "21400\n",
      "21500\n",
      "21600\n",
      "21700\n",
      "21800\n",
      "21900\n",
      "22000\n",
      "22100\n",
      "22200\n",
      "22300\n",
      "22400\n",
      "22500\n",
      "22600\n",
      "22700\n",
      "22800\n",
      "22900\n",
      "23000\n",
      "23100\n",
      "23200\n",
      "23300\n",
      "23400\n",
      "23500\n",
      "23600\n",
      "23700\n",
      "23800\n",
      "23900\n",
      "24000\n",
      "24100\n",
      "24200\n",
      "24300\n",
      "24400\n",
      "24500\n",
      "24600\n",
      "24700\n",
      "24800\n",
      "24900\n",
      "25000\n",
      "25100\n",
      "25200\n",
      "25300\n",
      "25400\n",
      "25500\n",
      "25600\n",
      "25700\n",
      "25800\n",
      "25900\n",
      "26000\n",
      "26100\n",
      "26200\n",
      "26300\n",
      "26400\n",
      "26500\n",
      "26600\n",
      "26700\n",
      "26800\n",
      "26900\n",
      "27000\n",
      "27100\n",
      "27200\n",
      "27300\n",
      "27400\n",
      "27500\n",
      "27600\n",
      "27700\n",
      "27800\n",
      "27900\n",
      "28000\n",
      "28100\n",
      "28200\n",
      "28300\n",
      "28400\n",
      "28500\n",
      "28600\n",
      "28700\n",
      "28800\n",
      "28900\n",
      "29000\n",
      "29100\n",
      "29200\n",
      "29300\n",
      "29400\n",
      "29500\n",
      "29600\n",
      "29700\n",
      "29800\n",
      "29900\n",
      "30000\n",
      "30100\n",
      "30200\n",
      "30300\n",
      "30400\n",
      "30500\n",
      "30600\n",
      "30700\n",
      "30800\n",
      "30900\n",
      "31000\n",
      "31100\n",
      "31200\n",
      "31300\n",
      "31400\n",
      "31500\n",
      "31600\n",
      "31700\n",
      "31800\n",
      "31900\n",
      "32000\n",
      "32100\n",
      "32200\n",
      "32300\n",
      "32400\n",
      "32500\n",
      "32600\n",
      "32700\n",
      "32800\n",
      "32900\n",
      "33000\n",
      "33100\n",
      "33200\n",
      "33300\n",
      "33400\n",
      "33500\n",
      "33600\n",
      "33700\n",
      "33800\n",
      "33900\n",
      "34000\n",
      "34100\n",
      "34200\n",
      "34300\n",
      "34400\n",
      "34500\n",
      "34600\n",
      "34700\n",
      "34800\n",
      "34900\n",
      "35000\n",
      "35100\n",
      "35200\n",
      "35300\n",
      "35400\n",
      "35500\n",
      "35600\n",
      "35700\n",
      "35800\n",
      "35900\n",
      "36000\n",
      "36100\n",
      "36200\n",
      "36300\n",
      "36400\n",
      "36500\n",
      "36600\n",
      "36700\n",
      "36800\n",
      "36900\n",
      "37000\n",
      "37100\n",
      "37200\n",
      "37300\n",
      "37400\n",
      "37500\n",
      "37600\n",
      "37700\n",
      "37800\n",
      "37900\n",
      "38000\n",
      "38100\n",
      "38200\n",
      "38300\n",
      "38400\n",
      "38500\n",
      "38600\n",
      "38700\n",
      "38800\n",
      "38900\n",
      "39000\n",
      "39100\n",
      "39200\n",
      "39300\n",
      "39400\n",
      "39500\n",
      "39600\n",
      "39700\n",
      "39800\n",
      "39900\n",
      "40000\n",
      "40100\n",
      "40200\n",
      "40300\n",
      "40400\n",
      "40500\n",
      "40600\n",
      "40700\n",
      "40800\n",
      "40900\n",
      "41000\n",
      "41100\n",
      "41200\n",
      "41300\n",
      "41400\n",
      "41500\n",
      "41600\n",
      "41700\n",
      "41800\n",
      "41900\n",
      "42000\n",
      "42100\n",
      "42200\n",
      "42300\n",
      "42400\n",
      "42500\n",
      "42600\n",
      "42700\n",
      "42800\n",
      "42900\n",
      "43000\n",
      "43100\n",
      "43200\n",
      "43300\n",
      "43400\n",
      "43500\n",
      "43600\n",
      "43700\n",
      "43800\n",
      "43900\n",
      "44000\n",
      "44100\n",
      "44200\n",
      "44300\n",
      "44400\n",
      "44500\n",
      "44600\n",
      "44700\n",
      "44800\n",
      "44900\n",
      "45000\n",
      "45100\n",
      "45200\n",
      "45300\n",
      "45400\n",
      "45500\n",
      "45600\n",
      "45700\n",
      "45800\n",
      "45900\n",
      "46000\n",
      "46100\n",
      "46200\n",
      "46300\n",
      "46400\n",
      "46500\n",
      "46600\n",
      "46700\n",
      "46800\n",
      "46900\n",
      "47000\n",
      "47100\n",
      "47200\n",
      "47300\n",
      "47400\n",
      "47500\n",
      "47600\n",
      "47700\n",
      "47800\n",
      "47900\n",
      "48000\n",
      "48100\n",
      "48200\n",
      "48300\n",
      "48400\n",
      "48500\n",
      "48600\n",
      "48700\n",
      "48800\n",
      "48900\n",
      "49000\n",
      "49100\n",
      "49200\n",
      "49300\n",
      "49400\n",
      "49500\n",
      "49600\n",
      "49700\n",
      "49800\n",
      "49900\n",
      "50000\n",
      "50100\n",
      "50200\n",
      "50300\n",
      "50400\n",
      "50500\n",
      "50600\n",
      "50700\n",
      "50800\n",
      "50900\n",
      "51000\n",
      "51100\n",
      "51200\n",
      "51300\n",
      "51400\n",
      "51500\n",
      "51600\n",
      "51700\n",
      "51800\n",
      "51900\n",
      "52000\n",
      "52100\n",
      "52200\n",
      "52300\n",
      "52400\n",
      "52500\n",
      "52600\n",
      "52700\n",
      "52800\n",
      "52900\n",
      "53000\n",
      "53100\n",
      "53200\n",
      "53300\n",
      "53400\n",
      "53500\n",
      "53600\n",
      "53700\n",
      "53800\n",
      "53900\n",
      "54000\n",
      "54100\n",
      "54200\n",
      "54300\n",
      "54400\n",
      "54500\n",
      "54600\n",
      "54700\n",
      "54800\n",
      "54900\n",
      "55000\n",
      "55100\n",
      "55200\n",
      "55300\n",
      "55400\n",
      "55500\n",
      "55600\n",
      "55700\n",
      "55800\n",
      "55900\n",
      "56000\n",
      "56100\n",
      "56200\n",
      "56300\n",
      "56400\n",
      "56500\n",
      "56600\n",
      "56700\n",
      "56800\n",
      "56900\n",
      "57000\n",
      "57100\n",
      "57200\n",
      "57300\n",
      "57400\n",
      "57500\n",
      "57600\n",
      "57700\n",
      "57800\n",
      "57900\n",
      "58000\n",
      "58100\n",
      "58200\n",
      "58300\n",
      "58400\n",
      "58500\n",
      "58600\n",
      "58700\n",
      "58800\n",
      "58900\n",
      "59000\n",
      "59100\n",
      "59200\n",
      "59300\n",
      "59400\n",
      "59500\n",
      "59600\n",
      "59700\n",
      "59800\n",
      "59900\n",
      "60000\n",
      "60100\n",
      "60200\n",
      "60300\n",
      "60400\n",
      "60500\n",
      "60600\n",
      "60700\n",
      "60800\n",
      "60900\n",
      "61000\n",
      "61100\n",
      "61200\n",
      "61300\n",
      "61400\n",
      "61500\n",
      "61600\n",
      "61700\n",
      "61800\n",
      "61900\n",
      "62000\n",
      "62100\n",
      "62200\n",
      "62300\n",
      "62400\n",
      "62500\n",
      "62600\n",
      "62700\n",
      "62800\n",
      "62900\n",
      "63000\n",
      "63100\n",
      "63200\n",
      "63300\n",
      "63400\n",
      "63500\n",
      "63600\n",
      "63700\n",
      "63800\n",
      "63900\n",
      "64000\n",
      "64100\n",
      "64200\n",
      "64300\n",
      "64400\n",
      "64500\n",
      "64600\n",
      "64700\n",
      "64800\n",
      "64900\n",
      "65000\n",
      "65100\n",
      "65200\n",
      "65300\n",
      "65400\n",
      "65500\n",
      "65600\n",
      "65700\n",
      "65800\n",
      "65900\n",
      "66000\n",
      "66100\n",
      "66200\n",
      "66300\n",
      "66400\n",
      "66500\n",
      "66600\n",
      "66700\n",
      "66800\n",
      "66900\n",
      "67000\n",
      "67100\n",
      "67200\n",
      "67300\n",
      "67400\n",
      "67500\n",
      "67600\n",
      "67700\n",
      "67800\n",
      "67900\n",
      "68000\n",
      "68100\n",
      "68200\n",
      "68300\n",
      "68400\n",
      "68500\n",
      "68600\n",
      "68700\n",
      "68800\n",
      "68900\n",
      "69000\n",
      "69100\n",
      "69200\n",
      "69300\n",
      "69400\n",
      "69500\n",
      "69600\n",
      "69700\n",
      "69800\n",
      "69900\n",
      "70000\n",
      "70100\n",
      "70200\n",
      "70300\n",
      "70400\n",
      "70500\n",
      "70600\n",
      "70700\n",
      "70800\n",
      "70900\n",
      "71000\n",
      "71100\n",
      "71200\n",
      "71300\n",
      "71400\n",
      "71500\n",
      "71600\n",
      "71700\n",
      "71800\n",
      "71900\n",
      "72000\n",
      "72100\n",
      "72200\n",
      "72300\n",
      "72400\n",
      "72500\n",
      "72600\n",
      "72700\n",
      "72800\n",
      "72900\n",
      "73000\n",
      "73100\n",
      "73200\n",
      "73300\n",
      "73400\n",
      "73500\n",
      "73600\n",
      "73700\n",
      "73800\n",
      "73900\n",
      "74000\n",
      "74100\n",
      "74200\n",
      "74300\n",
      "74400\n",
      "74500\n",
      "74600\n",
      "74700\n",
      "74800\n",
      "74900\n",
      "75000\n",
      "75100\n",
      "75200\n",
      "75300\n",
      "75400\n",
      "75500\n",
      "75600\n",
      "75700\n",
      "75800\n",
      "75900\n",
      "76000\n",
      "76100\n",
      "76200\n",
      "76300\n",
      "76400\n",
      "76500\n",
      "76600\n",
      "76700\n",
      "76800\n",
      "76900\n",
      "77000\n",
      "77100\n",
      "77200\n",
      "77300\n",
      "77400\n",
      "77500\n",
      "77600\n",
      "77700\n",
      "77800\n",
      "77900\n",
      "78000\n",
      "78100\n",
      "78200\n",
      "78300\n",
      "78400\n",
      "78500\n",
      "78600\n",
      "78700\n",
      "78800\n",
      "78900\n",
      "79000\n",
      "79100\n",
      "79200\n",
      "79300\n",
      "79400\n",
      "79500\n",
      "79600\n",
      "79700\n",
      "79800\n",
      "79900\n",
      "80000\n",
      "80100\n",
      "80200\n",
      "80300\n",
      "80400\n",
      "80500\n",
      "80600\n",
      "80700\n",
      "80800\n",
      "80900\n",
      "81000\n",
      "81100\n",
      "81200\n",
      "81300\n",
      "81400\n",
      "81500\n",
      "81600\n",
      "81700\n",
      "81800\n",
      "81900\n",
      "82000\n",
      "82100\n",
      "82200\n",
      "82300\n",
      "82400\n",
      "82500\n",
      "82600\n",
      "82700\n",
      "82800\n",
      "82900\n",
      "83000\n",
      "83100\n",
      "83200\n",
      "83300\n",
      "83400\n",
      "83500\n",
      "83600\n",
      "83700\n",
      "83800\n",
      "83900\n",
      "84000\n",
      "84100\n",
      "84200\n",
      "84300\n",
      "84400\n",
      "84500\n",
      "84600\n",
      "84700\n",
      "84800\n",
      "84900\n",
      "85000\n",
      "85100\n",
      "85200\n",
      "85300\n",
      "85400\n",
      "85500\n",
      "85600\n",
      "85700\n",
      "85800\n",
      "85900\n",
      "86000\n",
      "86100\n",
      "86200\n",
      "86300\n",
      "86400\n",
      "86500\n",
      "86600\n",
      "86700\n",
      "86800\n",
      "86900\n",
      "87000\n",
      "87100\n",
      "87200\n",
      "87300\n",
      "87400\n",
      "87500\n",
      "87600\n",
      "87700\n",
      "87800\n",
      "87900\n",
      "88000\n",
      "88100\n",
      "88200\n",
      "88300\n",
      "88400\n",
      "88500\n",
      "88600\n",
      "88700\n",
      "88800\n",
      "88900\n",
      "89000\n",
      "89100\n",
      "89200\n",
      "89300\n",
      "89400\n",
      "89500\n",
      "89600\n",
      "89700\n",
      "89800\n",
      "89900\n",
      "90000\n",
      "90100\n",
      "90200\n",
      "90300\n",
      "90400\n",
      "90500\n",
      "90600\n",
      "90700\n",
      "90800\n",
      "90900\n",
      "91000\n",
      "91100\n",
      "91200\n",
      "91300\n",
      "91400\n",
      "91500\n",
      "91600\n",
      "91700\n",
      "91800\n",
      "91900\n",
      "92000\n",
      "92100\n",
      "92200\n",
      "92300\n",
      "92400\n",
      "92500\n",
      "92600\n",
      "92700\n",
      "92800\n",
      "92900\n",
      "93000\n",
      "93100\n",
      "93200\n",
      "93300\n",
      "93400\n",
      "93500\n",
      "93600\n",
      "93700\n",
      "93800\n",
      "93900\n",
      "94000\n",
      "94100\n",
      "94200\n",
      "94300\n",
      "94400\n",
      "94500\n",
      "94600\n",
      "94700\n",
      "94800\n",
      "94900\n",
      "95000\n",
      "95100\n",
      "95200\n",
      "95300\n",
      "95400\n",
      "95500\n",
      "95600\n",
      "95700\n",
      "95800\n",
      "95900\n",
      "96000\n",
      "96100\n",
      "96200\n",
      "96300\n",
      "96400\n",
      "96500\n",
      "96600\n",
      "96700\n",
      "96800\n",
      "96900\n",
      "97000\n",
      "97100\n",
      "97200\n",
      "97300\n",
      "97400\n",
      "97500\n",
      "97600\n",
      "97700\n",
      "97800\n",
      "97900\n",
      "98000\n",
      "98100\n",
      "98200\n",
      "98300\n",
      "98400\n",
      "98500\n",
      "98600\n",
      "98700\n",
      "98800\n",
      "98900\n",
      "99000\n",
      "99100\n",
      "99200\n",
      "99300\n",
      "99400\n",
      "99500\n",
      "99600\n",
      "99700\n",
      "99800\n",
      "99900\n",
      "100000\n",
      "100100\n",
      "100200\n",
      "100300\n",
      "100400\n",
      "100500\n",
      "100600\n",
      "100700\n",
      "100800\n",
      "100900\n",
      "101000\n",
      "101100\n",
      "101200\n",
      "101300\n",
      "101400\n",
      "101500\n",
      "101600\n",
      "101700\n",
      "101800\n",
      "101900\n",
      "102000\n",
      "102100\n",
      "102200\n",
      "102300\n",
      "102400\n",
      "102500\n",
      "102600\n",
      "102700\n",
      "102800\n",
      "102900\n",
      "103000\n",
      "103100\n",
      "103200\n",
      "103300\n",
      "103400\n",
      "103500\n",
      "103600\n",
      "103700\n",
      "103800\n",
      "103900\n",
      "104000\n",
      "104100\n",
      "104200\n",
      "104300\n",
      "104400\n",
      "104500\n",
      "104600\n",
      "104700\n",
      "104800\n",
      "104900\n",
      "105000\n",
      "105100\n",
      "105200\n",
      "105300\n",
      "105400\n",
      "105500\n",
      "105600\n",
      "('go_6223', 0.9368700265251989, 0.9899976849555969)\n"
     ]
    }
   ],
   "source": [
    "#len(col_to_prob), len(col_to_label)\n",
    "from sklearn.metrics import roc_curve, f1_score, accuracy_score\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(col_to_label, col_to_prob, pos_label=1,drop_intermediate=True)\n",
    "prev = None\n",
    "accuracy_scores = []\n",
    "f1_thre = []\n",
    "\n",
    "print(len(thresholds))\n",
    "for index, thresh in enumerate(thresholds):\n",
    "    \n",
    "    if index %1000 == 0:\n",
    "        print(index)\n",
    "        \n",
    "    if prev is not None:\n",
    "        if abs(prev - thresh ) <= 0.01:\n",
    "            accuracy_scores.append(-1)\n",
    "            continue \n",
    "                    \n",
    "    accuracy_scores.append(f1_score(col_to_label,  [1 if m > thresh else 0 for m in col_to_prob]))\n",
    "    \n",
    "    prev = thresh\n",
    "        \n",
    "accuracies = np.array(accuracy_scores)\n",
    "max_accuracy = accuracies.max() \n",
    "max_accuracy_threshold =  thresholds[accuracies.argmax()]\n",
    "index = accuracies.argmax()\n",
    "f1_thre.append((Y.columns.tolist()[label_index], accuracies[index], thresholds[index]))\n",
    "    \n",
    "print((Y.columns.tolist()[label_index], accuracies[index], thresholds[index]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_to_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((array([   153,    303,    398, ..., 758501, 758519, 758558]),),\n",
       " (array([   130,    153,    267, ..., 758501, 758519, 758558]),))"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(np.asarray(all_scores) > 0.9),np.where(np.asarray(all_labels) > 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    all_labels, all_predictions, all_scores = calcStat(model, 0.8, 3, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.31197511664074645"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "f1_score(all_labels, all_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22279"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "false_positive_rate, true_positive_rate, thresholds = roc_curve(all_labels, all_scores, pos_label=1,drop_intermediate=True)\n",
    "len(thresholds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1011537, 245)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'go_105'"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59\n",
      "1\n",
      "2\n",
      "3\n",
      "59599\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-97-0a90ded93da5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             accuracy_scores.append(f1_score(all_labels, \n\u001b[0;32m---> 26\u001b[0;31m                                                  [1 if m > thresh else 0 for m in all_scores]))\n\u001b[0m\u001b[1;32m     27\u001b[0m             \u001b[0mprev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mthresh\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36mf1_score\u001b[0;34m(y_true, y_pred, labels, pos_label, average, sample_weight)\u001b[0m\n\u001b[1;32m   1057\u001b[0m     return fbeta_score(y_true, y_pred, 1, labels=labels,\n\u001b[1;32m   1058\u001b[0m                        \u001b[0mpos_label\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpos_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maverage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1059\u001b[0;31m                        sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m   1060\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1061\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36mfbeta_score\u001b[0;34m(y_true, y_pred, beta, labels, pos_label, average, sample_weight)\u001b[0m\n\u001b[1;32m   1180\u001b[0m                                                  \u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maverage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1181\u001b[0m                                                  \u001b[0mwarn_for\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'f-score'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1182\u001b[0;31m                                                  sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m   1183\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36mprecision_recall_fscore_support\u001b[0;34m(y_true, y_pred, beta, labels, pos_label, average, warn_for, sample_weight)\u001b[0m\n\u001b[1;32m   1413\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"beta should be >0 in the F-beta score\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1414\u001b[0m     labels = _check_set_wise_labels(y_true, y_pred, average, labels,\n\u001b[0;32m-> 1415\u001b[0;31m                                     pos_label)\n\u001b[0m\u001b[1;32m   1416\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m     \u001b[0;31m# Calculate tp_sum, pred_sum, true_sum ###\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36m_check_set_wise_labels\u001b[0;34m(y_true, y_pred, average, labels, pos_label)\u001b[0m\n\u001b[1;32m   1237\u001b[0m                          str(average_options))\n\u001b[1;32m   1238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1239\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1240\u001b[0m     \u001b[0mpresent_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munique_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0maverage\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'binary'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"binary\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"multiclass\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0my_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolumn_or_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolumn_or_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"binary\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m             \u001b[0munique_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munion1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcolumn_or_1d\u001b[0;34m(y, warn)\u001b[0m\n\u001b[1;32m    749\u001b[0m     \u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 751\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    752\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwarn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mravel\u001b[0;34m(a, order)\u001b[0m\n\u001b[1;32m   1685\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1686\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1687\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0masanyarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1688\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36masanyarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m    589\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m     \"\"\"\n\u001b[0;32m--> 591\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    592\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_curve, f1_score, accuracy_score\n",
    "\n",
    "f1_thre = []\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    \n",
    "    for label_index in range(y_train.shape[1]):\n",
    "        print(feature_index)\n",
    "        print(1)\n",
    "        all_labels, all_predictions, all_scores = calcStat(model, 0.5, label_index, train_loader)\n",
    "        print(2)\n",
    "\n",
    "        fpr, tpr, thresholds = roc_curve(all_labels, all_scores, pos_label=1,drop_intermediate=True)\n",
    "        print(3)\n",
    "        accuracy_scores = []\n",
    "\n",
    "        prev = None\n",
    "        print(len(thresholds))\n",
    "        for index, thresh in enumerate(thresholds):\n",
    "            if prev is not None:\n",
    "                if abs(prev - thresh ) <= 0.01:\n",
    "                    accuracy_scores.append(-1)\n",
    "                    continue \n",
    "\n",
    "            accuracy_scores.append(f1_score(all_labels, \n",
    "                                                 [1 if m > thresh else 0 for m in all_scores]))\n",
    "            prev = thresh\n",
    "        print(4)\n",
    "        accuracies = np.array(accuracy_scores)\n",
    "        max_accuracy = accuracies.max() \n",
    "        max_accuracy_threshold =  thresholds[accuracies.argmax()]\n",
    "\n",
    "        index = accuracies.argmax()\n",
    "        \n",
    "        f1_thre.append((Y.columns.tolist()[label_index], accuracies[index], thresholds[index]))\n",
    "        print((Y.columns.tolist()[label_index], accuracies[index], thresholds[index]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7971210479736328"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_accuracy_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1.99999988e+00, 9.99999881e-01, 9.99997854e-01, ...,\n",
       "        3.59954199e-39, 2.94180892e-39, 0.00000000e+00]),\n",
       " array([0.        , 0.        , 0.        , ..., 0.94782664, 0.94811543,\n",
       "        1.        ]),\n",
       " array([0.00000000e+00, 9.35891437e-05, 5.61534862e-04, ...,\n",
       "        1.00000000e+00, 1.00000000e+00, 1.00000000e+00]))"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thresholds,false_positive_rate,true_positive_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "false_positive_rate, true_positive_rate, thresholds = roc_curve(all_labels, all_predictions, pos_label=1)\n",
    "roc_auc = auc(false_positive_rate, true_positive_rate)\n",
    "\n",
    "\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "\n",
    "plt.plot(false_positive_rate, true_positive_rate, 'b',\n",
    "label='AUC = %0.2f'% roc_auc)\n",
    "plt.legend(loc='lower right')\n",
    "plt.plot([0,1],[0,1],'r--')\n",
    "plt.xlim([-0.1,1.2])\n",
    "plt.ylim([-0.1,1.2])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.00000000e+00, 1.00000000e+00, 9.99999881e-01, ...,\n",
       "       8.60676956e-39, 2.94174166e-39, 0.00000000e+00])"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "all_scores\n",
    "f1_score(all_labels, all_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([2.00000000e+00, 1.00000000e+00, 9.99999881e-01, ...,\n",
       "        8.60676956e-39, 2.94174166e-39, 0.00000000e+00]),\n",
       " array([0.        , 0.        , 0.        , ..., 0.99183417, 0.99220764,\n",
       "        1.        ]))"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thresholds,false_positive_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9757491237759556"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(all_labels, all_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "# fpr = dict()\n",
    "# tpr = dict()\n",
    "# roc_auc = dict()\n",
    "# for i in range(2):\n",
    "#     fpr[i], tpr[i], _ = roc_curve(all_labels, all_predictions)\n",
    "#     roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "fpr, tpr ,_= roc_curve(all_labels, all_scores)\n",
    "\n",
    "plt.figure()\n",
    "# plt.plot(fpr[1], tpr[1])\n",
    "plt.plot(fpr, tpr)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "758653"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7022734004739337"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "#accuracy_score(all_labels, all_predictions)\n",
    "f1_score(all_labels, all_predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0'))"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# probs = probs.cpu().numpy()\n",
    "\n",
    "results = np.zeros(shape=probs[:, 5].shape)\n",
    "results[np.where(probs[:, 5] >= 0.6)] = 1\n",
    "results,labels[:,5]\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1.], device='cuda:0')"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[:, 5][[7, 24, 80]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "labels[:, 5][[7, 24, 80]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.6646, 0.9965, 0.9969], device='cuda:0')"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs[:, 5][[7, 24, 80]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 7, 18, 24, 59, 61, 80]),)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(probs.cpu().numpy()[:, 5] >= 0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9989, device='cuda:0')"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs[:, 4][85]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.09679476e-07, 2.90676667e-07, 3.91447611e-05, 2.91655144e-08,\n",
       "       1.25464634e-04, 4.90657476e-05, 1.32901849e-12, 3.30286234e-08,\n",
       "       1.69566438e-01, 2.47759605e-03, 1.94958920e-05, 4.36225060e-15,\n",
       "       7.10347422e-06, 3.10336276e-07, 4.69285505e-12, 6.62128116e-07,\n",
       "       1.55898991e-08, 1.00924335e-05, 1.75348471e-03, 8.74051638e-03,\n",
       "       1.51977622e-06, 3.25389031e-08, 9.51959578e-07, 1.66297596e-05,\n",
       "       1.03913878e-09, 2.36632415e-11, 2.63910799e-04, 5.67465008e-09,\n",
       "       4.17529522e-09, 1.64859084e-04, 5.49246648e-10, 3.19015512e-13,\n",
       "       4.75896150e-01, 1.72227337e-05, 1.14775212e-05, 1.04977180e-05,\n",
       "       1.14273244e-05, 1.70626707e-14, 2.02233716e-12, 1.20797682e-04,\n",
       "       2.62322303e-10, 8.05840854e-14, 6.63057872e-05, 1.68745589e-06,\n",
       "       2.28975496e-05, 1.80310258e-06, 1.04832306e-11, 2.95677921e-03,\n",
       "       6.69398014e-06, 4.13345769e-02, 2.13086295e-07, 1.67253620e-05,\n",
       "       3.35031981e-03, 1.79383193e-08, 7.44296904e-06, 7.14817205e-10,\n",
       "       9.99688625e-01, 2.12350432e-02, 1.84505453e-07, 1.26406724e-06,\n",
       "       9.66330990e-03, 3.96641517e-05, 1.40595925e-03, 5.87567115e-08,\n",
       "       2.10742598e-07, 5.31163437e-07, 1.38904850e-34, 2.17390974e-04,\n",
       "       5.28562523e-04, 4.71217572e-06, 4.37411573e-03, 1.38706705e-02,\n",
       "       2.09760403e-10, 5.41748595e-05, 6.19980347e-05, 5.07973926e-14,\n",
       "       7.96295411e-04, 9.99982476e-01, 5.19073494e-02, 6.68910403e-08,\n",
       "       1.94035738e-06, 6.03738986e-03, 1.51745738e-07, 7.04935155e-06,\n",
       "       4.70601430e-04, 9.99910712e-01, 3.26805716e-09, 3.22677603e-04,\n",
       "       5.24821020e-10, 3.48107071e-10, 6.27278423e-05, 1.80135444e-08,\n",
       "       1.36570394e-01, 1.08543452e-06, 1.31289960e-06, 1.60537491e-06,\n",
       "       1.11531070e-03, 1.18163433e-02, 4.93989412e-14, 2.74207006e-04],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs.cpu().numpy()[:, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([52, 77, 83, 96]),)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(probs.cpu().numpy()[:, 3] >= 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.4866, device='cuda:0')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4840, 0.4840, 0.4840, 0.4840, 0.4840, 0.4840, 0.4840, 0.4840, 0.4840,\n",
       "        0.4840, 0.4840, 0.4840, 0.4840, 0.4840, 0.4840, 0.4840, 0.4840, 0.4840,\n",
       "        0.4840, 0.4840, 0.4840, 0.4840, 0.4840, 0.4840, 0.4840, 0.4840, 0.4840,\n",
       "        0.4840, 0.4840, 0.4840, 0.4840, 0.4840, 0.4840, 0.4840, 0.4840, 0.4840,\n",
       "        0.4840, 0.4840, 0.4840, 0.4840, 0.4840, 0.4840, 0.4840, 0.4840, 0.4840,\n",
       "        0.4840, 0.4840, 0.4840, 0.4840, 0.4840, 0.4840, 0.4840, 0.4840, 0.4840,\n",
       "        0.4840, 0.4840, 0.4840, 0.4840, 0.4840, 0.4840, 0.4840, 0.4840, 0.4840,\n",
       "        0.4840, 0.4840, 0.4840, 0.4840, 0.4840, 0.4840, 0.4840, 0.4840, 0.4840,\n",
       "        0.4840, 0.4840, 0.4840, 0.4840, 0.4840, 0.4840, 0.4840, 0.4840, 0.4840,\n",
       "        0.4840, 0.4840, 0.4840, 0.4840, 0.4840, 0.4840, 0.4840, 0.4840, 0.4840,\n",
       "        0.4840, 0.4840, 0.4840, 0.4840, 0.4840, 0.4840, 0.4840, 0.4840, 0.4840,\n",
       "        0.4840], device='cuda:0')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs[:, 12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.0430, device='cuda:0'), tensor(0.0256, device='cuda:0'))"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0550, 0.0236, 0.0256, 0.0304, 0.0132, 0.0555, 0.0071, 0.0104, 0.0280,\n",
      "        0.0525, 0.0256, 0.0264, 0.0260, 0.0304, 0.0270, 0.0317, 0.0666, 0.0278,\n",
      "        0.0091, 0.0278, 0.0306, 0.0254, 0.0029, 0.0238, 0.0287, 0.1074, 0.0277,\n",
      "        0.0156, 0.0297, 0.0277, 0.0430, 0.0097, 0.0268, 0.0294, 0.0268, 0.0159,\n",
      "        0.0388, 0.0305, 0.0160, 0.0125, 0.0223, 0.0308, 0.0283, 0.0278, 0.0182,\n",
      "        0.0286, 0.0275, 0.0269, 0.0300, 0.0020, 0.0554, 0.0076, 0.0279, 0.0260,\n",
      "        0.0320, 0.0382, 0.0279, 0.0228, 0.0251, 0.0290, 0.0303, 0.0226, 0.0314,\n",
      "        0.0286, 0.0256, 0.0281, 0.0133, 0.0081, 0.0042, 0.0139, 0.0464, 0.0308,\n",
      "        0.0265, 0.0310, 0.0228, 0.0182, 0.0375, 0.0188, 0.0466, 0.0239, 0.0280,\n",
      "        0.0282, 0.0787, 0.0279, 0.0285, 0.0215, 0.0254, 0.0096, 0.0250, 0.0240,\n",
      "        0.0595, 0.0332, 0.0151, 0.0266, 0.0230, 0.0101, 0.0197, 0.0380, 0.0290,\n",
      "        0.0260], device='cuda:0')\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "for feature in range(probs.shape[1]):\n",
    "    print(probs[:, 5])\n",
    "    print(labels[:, 5])\n",
    "    \n",
    "    break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(model, device, test_loader, epoch, writer, criterion):\n",
    "    model.eval()\n",
    "    running_correct = 0.0\n",
    "    running_loss = 0.0\n",
    "    calc_count = 0.0\n",
    "\n",
    "    per_label_expected = None\n",
    "    per_label_predicted = None\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            \n",
    "            images = images.float().to(device)\n",
    "            labels = labels.float().to(device)\n",
    "\n",
    "            images = Variable(images)\n",
    "            labels = Variable(labels)\n",
    "            outputs = model(images)\n",
    "            probs = torch.sigmoid(outputs)\n",
    "            \n",
    "            \n",
    "\n",
    "            #_, pred = torch.max(outputs.data, 1)\n",
    "            loss = criterion(outputs, labels)\n",
    "            calc_count += outputs.data.shape[0]\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "    print('validation loss {0}'.format(running_loss/calc_count))\n",
    "    \n",
    "    return running_loss/calc_count"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
